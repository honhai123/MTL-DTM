{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f25d0615-f90c-497b-9bba-4f470dc163db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d4306ea-292a-418a-ac86-d18e029ea1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ReadTxtIntoDataset(folder_path):\n",
    "    folder_path = folder_path\n",
    "    # 获取文件夹中所有文件的列表\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    x_list = []\n",
    "    # 读取每个文件中的数据\n",
    "    data_list = []\n",
    "    for file_name in files:\n",
    "        # print(file_name)\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        # 读取文本文件\n",
    "        data = pd.read_csv(file_path,  delimiter=',', skipinitialspace=True, header=0)\n",
    "        # 清理列名中的空格\n",
    "        data.columns = data.columns.str.strip()\n",
    "        x_values = (data['MISES'][72616],data['MISES'][178908].astype(float),data['MISES'][128977].astype(float)) #提取指定的地方做传感器的输入\n",
    "        x_list.append(x_values)\n",
    "        # print(data.head())\n",
    "        # 提取第二列数据\n",
    "        second_column = data['MISES'].astype(float).values\n",
    "        # 打印提取的第二列数据\n",
    "        # print(\"Second column values:\", second_column)\n",
    "        data_list.append(second_column)\n",
    "    \n",
    "    data = torch.tensor(x_list).float()  \n",
    "    label =  torch.tensor(np.array(data_list)).float()\n",
    "\n",
    "    # 打印划分后的数据\n",
    "    return data,label\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return torch.mean((y_true - y_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2448f9-59d4-452a-9533-dcd1be5b286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录： /root/autodl-tmp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"当前工作目录：\", current_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecaf145b-e93c-4839-bfb7-8fe22701ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"/root/autodl-fs/ODB-E\"\n",
    "data,label = ReadTxtIntoDataset(folder_path)\n",
    "data,label = data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e564a18-db05-4dd3-99bb-e945bc2415f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 40.1373,   1.6524, 139.7228],\n",
      "        [ 56.3801,   2.2885, 192.1463],\n",
      "        [103.6645,   4.1334, 349.2931],\n",
      "        ...,\n",
      "        [ 99.1270,   3.9372, 331.8474],\n",
      "        [ 24.1407,   1.0321,  87.3307],\n",
      "        [ 92.2133,   3.7398, 314.4116]])\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80bd30b6-b32e-43bb-885e-a53c8365569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_dataset(Dataset):\n",
    "    def __init__(self,data,labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "           \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx],self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0a50634-f5fe-4278-81f0-0ad78d9c82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, train_labels, val_labels = train_test_split(data, label, test_size=0.2, random_state=42)\n",
    "train_dataset = My_dataset(train_data, train_labels)\n",
    "val_dataset = My_dataset(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6534b302-e26c-4f0b-aefc-b233a708466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_size,hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size,output_size),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc4f0a0f-99da-4956-8952-80dab93cafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 3\n",
    "hidden = 2000\n",
    "out = 245293\n",
    "model = MLP(input, hidden, out)\n",
    "batch_size = 16\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c290fbd3-2b6c-4b8b-9484-57e29be86c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()  # 均方误差损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00437556-da0f-47a6-a1d8-cf861cd2be81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('MLP_E_model_weights.pth202434'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c31f5b8-ce7f-4072-8e4e-462120e91c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf661756-4fb2-4824-96b8-8ba7d0121d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.6341e-03, 1.8623e-04, 1.5719e-02],\n",
      "        [6.2350e-03, 2.5039e-04, 2.0962e-02],\n",
      "        [4.5156e-03, 1.8640e-04, 1.5720e-02],\n",
      "        ...,\n",
      "        [5.2078e-03, 2.0881e-04, 1.7469e-02],\n",
      "        [1.9952e-04, 3.9523e-06, 3.8282e-06],\n",
      "        [4.5158e-03, 1.8692e-04, 1.5721e-02]])\n",
      "tensor([[4.0177e-03, 3.5147e-03, 4.0200e-03,  ..., 4.1771e-03, 4.2291e-03,\n",
      "         4.0865e-03],\n",
      "        [5.3581e-03, 4.6873e-03, 5.3611e-03,  ..., 5.5679e-03, 5.6371e-03,\n",
      "         5.4470e-03],\n",
      "        [4.0169e-03, 3.5139e-03, 4.0191e-03,  ..., 4.1774e-03, 4.2294e-03,\n",
      "         4.0867e-03],\n",
      "        ...,\n",
      "        [4.4652e-03, 3.9062e-03, 4.4678e-03,  ..., 4.6398e-03, 4.6974e-03,\n",
      "         4.5391e-03],\n",
      "        [1.5400e-06, 1.5908e-06, 1.5772e-06,  ..., 6.6784e-07, 6.3492e-07,\n",
      "         7.3020e-07],\n",
      "        [4.0171e-03, 3.5141e-03, 4.0193e-03,  ..., 4.1770e-03, 4.2290e-03,\n",
      "         4.0864e-03]])\n"
     ]
    }
   ],
   "source": [
    "print(val_data)\n",
    "print(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "658e78e5-3d3f-459c-82fa-8e1a91c86ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [10/127], Loss: 0.00614202\n",
      "Epoch [1/100], Step [20/127], Loss: 0.01074389\n",
      "Epoch [1/100], Step [30/127], Loss: 0.00691102\n",
      "Epoch [1/100], Step [40/127], Loss: 0.01616484\n",
      "Epoch [1/100], Step [50/127], Loss: 0.01307197\n",
      "Epoch [1/100], Step [60/127], Loss: 0.01807889\n",
      "Epoch [1/100], Step [70/127], Loss: 0.00999989\n",
      "Epoch [1/100], Step [80/127], Loss: 0.00831980\n",
      "Epoch [1/100], Step [90/127], Loss: 0.00620721\n",
      "Epoch [1/100], Step [100/127], Loss: 0.00604198\n",
      "Epoch [1/100], Step [110/127], Loss: 0.00864676\n",
      "Epoch [1/100], Step [120/127], Loss: 0.00817076\n",
      "Epoch 1/100: Val Loss: 0.20072010\n",
      "Epoch [2/100], Step [10/127], Loss: 0.00851715\n",
      "Epoch [2/100], Step [20/127], Loss: 0.00661816\n",
      "Epoch [2/100], Step [30/127], Loss: 0.00866277\n",
      "Epoch [2/100], Step [40/127], Loss: 0.00662009\n",
      "Epoch [2/100], Step [50/127], Loss: 0.02223987\n",
      "Epoch [2/100], Step [60/127], Loss: 0.01298276\n",
      "Epoch [2/100], Step [70/127], Loss: 0.00953603\n",
      "Epoch [2/100], Step [80/127], Loss: 0.01430227\n",
      "Epoch [2/100], Step [90/127], Loss: 0.00830772\n",
      "Epoch [2/100], Step [100/127], Loss: 0.01410664\n",
      "Epoch [2/100], Step [110/127], Loss: 0.04005891\n",
      "Epoch [2/100], Step [120/127], Loss: 0.08555873\n",
      "Epoch 2/100: Val Loss: 0.19171488\n",
      "Epoch [3/100], Step [10/127], Loss: 0.03857744\n",
      "Epoch [3/100], Step [20/127], Loss: 0.05085111\n",
      "Epoch [3/100], Step [30/127], Loss: 0.03152045\n",
      "Epoch [3/100], Step [40/127], Loss: 0.07136682\n",
      "Epoch [3/100], Step [50/127], Loss: 0.01750979\n",
      "Epoch [3/100], Step [60/127], Loss: 0.01056292\n",
      "Epoch [3/100], Step [70/127], Loss: 0.02464228\n",
      "Epoch [3/100], Step [80/127], Loss: 0.01883928\n",
      "Epoch [3/100], Step [90/127], Loss: 0.00889545\n",
      "Epoch [3/100], Step [100/127], Loss: 0.01184151\n",
      "Epoch [3/100], Step [110/127], Loss: 0.00950626\n",
      "Epoch [3/100], Step [120/127], Loss: 0.02924539\n",
      "Epoch 3/100: Val Loss: 0.21062046\n",
      "Epoch [4/100], Step [10/127], Loss: 0.26731679\n",
      "Epoch [4/100], Step [20/127], Loss: 0.16590255\n",
      "Epoch [4/100], Step [30/127], Loss: 0.03070915\n",
      "Epoch [4/100], Step [40/127], Loss: 0.03420661\n",
      "Epoch [4/100], Step [50/127], Loss: 0.74769419\n",
      "Epoch [4/100], Step [60/127], Loss: 0.56288743\n",
      "Epoch [4/100], Step [70/127], Loss: 0.08532663\n",
      "Epoch [4/100], Step [80/127], Loss: 0.42319685\n",
      "Epoch [4/100], Step [90/127], Loss: 4.98319149\n",
      "Epoch [4/100], Step [100/127], Loss: 2.18197179\n",
      "Epoch [4/100], Step [110/127], Loss: 0.39459103\n",
      "Epoch [4/100], Step [120/127], Loss: 0.44935638\n",
      "Epoch 4/100: Val Loss: 0.21721899\n",
      "Epoch [5/100], Step [10/127], Loss: 0.20340021\n",
      "Epoch [5/100], Step [20/127], Loss: 1.73503721\n",
      "Epoch [5/100], Step [30/127], Loss: 0.10038280\n",
      "Epoch [5/100], Step [40/127], Loss: 0.14353046\n",
      "Epoch [5/100], Step [50/127], Loss: 0.04972455\n",
      "Epoch [5/100], Step [60/127], Loss: 0.02542653\n",
      "Epoch [5/100], Step [70/127], Loss: 0.02906314\n",
      "Epoch [5/100], Step [80/127], Loss: 0.02933573\n",
      "Epoch [5/100], Step [90/127], Loss: 0.01469704\n",
      "Epoch [5/100], Step [100/127], Loss: 0.01874130\n",
      "Epoch [5/100], Step [110/127], Loss: 0.08610526\n",
      "Epoch [5/100], Step [120/127], Loss: 0.20428200\n",
      "Epoch 5/100: Val Loss: 0.18063400\n",
      "Epoch [6/100], Step [10/127], Loss: 0.10781381\n",
      "Epoch [6/100], Step [20/127], Loss: 1.96447611\n",
      "Epoch [6/100], Step [30/127], Loss: 0.20929871\n",
      "Epoch [6/100], Step [40/127], Loss: 3.06014252\n",
      "Epoch [6/100], Step [50/127], Loss: 0.66664910\n",
      "Epoch [6/100], Step [60/127], Loss: 4.98691368\n",
      "Epoch [6/100], Step [70/127], Loss: 15.08909607\n",
      "Epoch [6/100], Step [80/127], Loss: 24.95956993\n",
      "Epoch [6/100], Step [90/127], Loss: 19.33886719\n",
      "Epoch [6/100], Step [100/127], Loss: 2.79451370\n",
      "Epoch [6/100], Step [110/127], Loss: 3.80060983\n",
      "Epoch [6/100], Step [120/127], Loss: 1.33884680\n",
      "Epoch 6/100: Val Loss: 0.17847155\n",
      "Epoch [7/100], Step [10/127], Loss: 0.77782387\n",
      "Epoch [7/100], Step [20/127], Loss: 0.20125635\n",
      "Epoch [7/100], Step [30/127], Loss: 0.37441906\n",
      "Epoch [7/100], Step [40/127], Loss: 0.19745828\n",
      "Epoch [7/100], Step [50/127], Loss: 0.12211662\n",
      "Epoch [7/100], Step [60/127], Loss: 0.17939506\n",
      "Epoch [7/100], Step [70/127], Loss: 0.26414257\n",
      "Epoch [7/100], Step [80/127], Loss: 0.25037810\n",
      "Epoch [7/100], Step [90/127], Loss: 0.68008202\n",
      "Epoch [7/100], Step [100/127], Loss: 0.30836955\n",
      "Epoch [7/100], Step [110/127], Loss: 0.11998858\n",
      "Epoch [7/100], Step [120/127], Loss: 0.11383197\n",
      "Epoch 7/100: Val Loss: 0.26674704\n",
      "Epoch [8/100], Step [10/127], Loss: 0.08687042\n",
      "Epoch [8/100], Step [20/127], Loss: 0.07689489\n",
      "Epoch [8/100], Step [30/127], Loss: 0.06680330\n",
      "Epoch [8/100], Step [40/127], Loss: 0.07136550\n",
      "Epoch [8/100], Step [50/127], Loss: 0.06870114\n",
      "Epoch [8/100], Step [60/127], Loss: 0.05146749\n",
      "Epoch [8/100], Step [70/127], Loss: 0.04314116\n",
      "Epoch [8/100], Step [80/127], Loss: 0.41628596\n",
      "Epoch [8/100], Step [90/127], Loss: 0.03654847\n",
      "Epoch [8/100], Step [100/127], Loss: 0.01878568\n",
      "Epoch [8/100], Step [110/127], Loss: 0.03687873\n",
      "Epoch [8/100], Step [120/127], Loss: 0.06008670\n",
      "Epoch 8/100: Val Loss: 1.82117074\n",
      "Epoch [9/100], Step [10/127], Loss: 0.02623510\n",
      "Epoch [9/100], Step [20/127], Loss: 0.01388275\n",
      "Epoch [9/100], Step [30/127], Loss: 0.03448746\n",
      "Epoch [9/100], Step [40/127], Loss: 0.24721242\n",
      "Epoch [9/100], Step [50/127], Loss: 0.74822754\n",
      "Epoch [9/100], Step [60/127], Loss: 0.15283452\n",
      "Epoch [9/100], Step [70/127], Loss: 0.05401863\n",
      "Epoch [9/100], Step [80/127], Loss: 0.06203973\n",
      "Epoch [9/100], Step [90/127], Loss: 0.03205361\n",
      "Epoch [9/100], Step [100/127], Loss: 0.47437000\n",
      "Epoch [9/100], Step [110/127], Loss: 0.26572973\n",
      "Epoch [9/100], Step [120/127], Loss: 3.03904104\n",
      "Epoch 9/100: Val Loss: 1.97803747\n",
      "Epoch [10/100], Step [10/127], Loss: 2.10456204\n",
      "Epoch [10/100], Step [20/127], Loss: 43.11949539\n",
      "Epoch [10/100], Step [30/127], Loss: 13.21090698\n",
      "Epoch [10/100], Step [40/127], Loss: 7.58047104\n",
      "Epoch [10/100], Step [50/127], Loss: 3.24730253\n",
      "Epoch [10/100], Step [60/127], Loss: 1.09496093\n",
      "Epoch [10/100], Step [70/127], Loss: 2.54578352\n",
      "Epoch [10/100], Step [80/127], Loss: 2.47954893\n",
      "Epoch [10/100], Step [90/127], Loss: 0.19776227\n",
      "Epoch [10/100], Step [100/127], Loss: 0.52436173\n",
      "Epoch [10/100], Step [110/127], Loss: 0.44316909\n",
      "Epoch [10/100], Step [120/127], Loss: 0.25414219\n",
      "Epoch 10/100: Val Loss: 0.36693258\n",
      "Epoch [11/100], Step [10/127], Loss: 0.60757947\n",
      "Epoch [11/100], Step [20/127], Loss: 0.11885537\n",
      "Epoch [11/100], Step [30/127], Loss: 0.63825244\n",
      "Epoch [11/100], Step [40/127], Loss: 0.05149043\n",
      "Epoch [11/100], Step [50/127], Loss: 0.27312675\n",
      "Epoch [11/100], Step [60/127], Loss: 1.95965672\n",
      "Epoch [11/100], Step [70/127], Loss: 0.76069039\n",
      "Epoch [11/100], Step [80/127], Loss: 0.96666193\n",
      "Epoch [11/100], Step [90/127], Loss: 2.94233537\n",
      "Epoch [11/100], Step [100/127], Loss: 1.71573663\n",
      "Epoch [11/100], Step [110/127], Loss: 0.91906726\n",
      "Epoch [11/100], Step [120/127], Loss: 0.11382954\n",
      "Epoch 11/100: Val Loss: 0.32148661\n",
      "Epoch [12/100], Step [10/127], Loss: 0.15874033\n",
      "Epoch [12/100], Step [20/127], Loss: 0.14217521\n",
      "Epoch [12/100], Step [30/127], Loss: 0.06028936\n",
      "Epoch [12/100], Step [40/127], Loss: 0.04758754\n",
      "Epoch [12/100], Step [50/127], Loss: 0.02633435\n",
      "Epoch [12/100], Step [60/127], Loss: 0.05186816\n",
      "Epoch [12/100], Step [70/127], Loss: 0.52935708\n",
      "Epoch [12/100], Step [80/127], Loss: 3.03667426\n",
      "Epoch [12/100], Step [90/127], Loss: 0.47302249\n",
      "Epoch [12/100], Step [100/127], Loss: 13.13764381\n",
      "Epoch [12/100], Step [110/127], Loss: 5.78395748\n",
      "Epoch [12/100], Step [120/127], Loss: 2.84270573\n",
      "Epoch 12/100: Val Loss: 0.38688922\n",
      "Epoch [13/100], Step [10/127], Loss: 0.58058351\n",
      "Epoch [13/100], Step [20/127], Loss: 0.23802389\n",
      "Epoch [13/100], Step [30/127], Loss: 0.09965803\n",
      "Epoch [13/100], Step [40/127], Loss: 0.06816945\n",
      "Epoch [13/100], Step [50/127], Loss: 0.05138701\n",
      "Epoch [13/100], Step [60/127], Loss: 0.02210654\n",
      "Epoch [13/100], Step [70/127], Loss: 0.05265643\n",
      "Epoch [13/100], Step [80/127], Loss: 0.04130196\n",
      "Epoch [13/100], Step [90/127], Loss: 0.01950261\n",
      "Epoch [13/100], Step [100/127], Loss: 0.01139113\n",
      "Epoch [13/100], Step [110/127], Loss: 0.02187620\n",
      "Epoch [13/100], Step [120/127], Loss: 0.13178594\n",
      "Epoch 13/100: Val Loss: 0.25337857\n",
      "Epoch [14/100], Step [10/127], Loss: 0.11259995\n",
      "Epoch [14/100], Step [20/127], Loss: 0.20874520\n",
      "Epoch [14/100], Step [30/127], Loss: 0.13152838\n",
      "Epoch [14/100], Step [40/127], Loss: 0.07613604\n",
      "Epoch [14/100], Step [50/127], Loss: 0.05472170\n",
      "Epoch [14/100], Step [60/127], Loss: 0.96358609\n",
      "Epoch [14/100], Step [70/127], Loss: 0.11620265\n",
      "Epoch [14/100], Step [80/127], Loss: 6.38545799\n",
      "Epoch [14/100], Step [90/127], Loss: 2.56958294\n",
      "Epoch [14/100], Step [100/127], Loss: 6.03937912\n",
      "Epoch [14/100], Step [110/127], Loss: 12.70122051\n",
      "Epoch [14/100], Step [120/127], Loss: 4.35772943\n",
      "Epoch 14/100: Val Loss: 0.24221980\n",
      "Epoch [15/100], Step [10/127], Loss: 2.31475115\n",
      "Epoch [15/100], Step [20/127], Loss: 2.35281634\n",
      "Epoch [15/100], Step [30/127], Loss: 0.11909203\n",
      "Epoch [15/100], Step [40/127], Loss: 0.30933487\n",
      "Epoch [15/100], Step [50/127], Loss: 0.30223238\n",
      "Epoch [15/100], Step [60/127], Loss: 0.20934439\n",
      "Epoch [15/100], Step [70/127], Loss: 0.06638696\n",
      "Epoch [15/100], Step [80/127], Loss: 0.03911731\n",
      "Epoch [15/100], Step [90/127], Loss: 0.09637935\n",
      "Epoch [15/100], Step [100/127], Loss: 0.02879119\n",
      "Epoch [15/100], Step [110/127], Loss: 0.06720900\n",
      "Epoch [15/100], Step [120/127], Loss: 0.01572574\n",
      "Epoch 15/100: Val Loss: 0.16843536\n",
      "Epoch [16/100], Step [10/127], Loss: 0.02196167\n",
      "Epoch [16/100], Step [20/127], Loss: 0.00940484\n",
      "Epoch [16/100], Step [30/127], Loss: 0.01030317\n",
      "Epoch [16/100], Step [40/127], Loss: 0.00978378\n",
      "Epoch [16/100], Step [50/127], Loss: 0.00994330\n",
      "Epoch [16/100], Step [60/127], Loss: 0.00760416\n",
      "Epoch [16/100], Step [70/127], Loss: 0.00483756\n",
      "Epoch [16/100], Step [80/127], Loss: 0.01079421\n",
      "Epoch [16/100], Step [90/127], Loss: 0.00657762\n",
      "Epoch [16/100], Step [100/127], Loss: 0.00440936\n",
      "Epoch [16/100], Step [110/127], Loss: 0.01499021\n",
      "Epoch [16/100], Step [120/127], Loss: 0.00840526\n",
      "Epoch 16/100: Val Loss: 0.16528381\n",
      "Epoch [17/100], Step [10/127], Loss: 0.00784896\n",
      "Epoch [17/100], Step [20/127], Loss: 0.01217475\n",
      "Epoch [17/100], Step [30/127], Loss: 0.01277976\n",
      "Epoch [17/100], Step [40/127], Loss: 0.02136278\n",
      "Epoch [17/100], Step [50/127], Loss: 0.01595553\n",
      "Epoch [17/100], Step [60/127], Loss: 0.01453574\n",
      "Epoch [17/100], Step [70/127], Loss: 0.02218761\n",
      "Epoch [17/100], Step [80/127], Loss: 0.01047133\n",
      "Epoch [17/100], Step [90/127], Loss: 0.01859426\n",
      "Epoch [17/100], Step [100/127], Loss: 0.00689685\n",
      "Epoch [17/100], Step [110/127], Loss: 0.00393443\n",
      "Epoch [17/100], Step [120/127], Loss: 0.00686012\n",
      "Epoch 17/100: Val Loss: 0.43253246\n",
      "Epoch [18/100], Step [10/127], Loss: 0.00473529\n",
      "Epoch [18/100], Step [20/127], Loss: 0.00592990\n",
      "Epoch [18/100], Step [30/127], Loss: 0.00413593\n",
      "Epoch [18/100], Step [40/127], Loss: 0.00483664\n",
      "Epoch [18/100], Step [50/127], Loss: 0.00961254\n",
      "Epoch [18/100], Step [60/127], Loss: 0.00710032\n",
      "Epoch [18/100], Step [70/127], Loss: 0.00328564\n",
      "Epoch [18/100], Step [80/127], Loss: 0.00510936\n",
      "Epoch [18/100], Step [90/127], Loss: 0.01651490\n",
      "Epoch [18/100], Step [100/127], Loss: 0.01089030\n",
      "Epoch [18/100], Step [110/127], Loss: 0.01334036\n",
      "Epoch [18/100], Step [120/127], Loss: 0.00675561\n",
      "Epoch 18/100: Val Loss: 0.15488452\n",
      "Epoch [19/100], Step [10/127], Loss: 0.04104586\n",
      "Epoch [19/100], Step [20/127], Loss: 0.06395613\n",
      "Epoch [19/100], Step [30/127], Loss: 0.02303207\n",
      "Epoch [19/100], Step [40/127], Loss: 0.04575993\n",
      "Epoch [19/100], Step [50/127], Loss: 0.02358528\n",
      "Epoch [19/100], Step [60/127], Loss: 0.00926033\n",
      "Epoch [19/100], Step [70/127], Loss: 0.01740676\n",
      "Epoch [19/100], Step [80/127], Loss: 0.00800794\n",
      "Epoch [19/100], Step [90/127], Loss: 0.02189589\n",
      "Epoch [19/100], Step [100/127], Loss: 0.00659012\n",
      "Epoch [19/100], Step [110/127], Loss: 0.01457524\n",
      "Epoch [19/100], Step [120/127], Loss: 0.00792572\n",
      "Epoch 19/100: Val Loss: 0.29166037\n",
      "Epoch [20/100], Step [10/127], Loss: 0.00522635\n",
      "Epoch [20/100], Step [20/127], Loss: 0.00657552\n",
      "Epoch [20/100], Step [30/127], Loss: 0.00406960\n",
      "Epoch [20/100], Step [40/127], Loss: 0.00372429\n",
      "Epoch [20/100], Step [50/127], Loss: 0.02238069\n",
      "Epoch [20/100], Step [60/127], Loss: 0.28629130\n",
      "Epoch [20/100], Step [70/127], Loss: 0.39250916\n",
      "Epoch [20/100], Step [80/127], Loss: 0.17124897\n",
      "Epoch [20/100], Step [90/127], Loss: 0.03155425\n",
      "Epoch [20/100], Step [100/127], Loss: 0.01178798\n",
      "Epoch [20/100], Step [110/127], Loss: 0.02033784\n",
      "Epoch [20/100], Step [120/127], Loss: 0.09027205\n",
      "Epoch 20/100: Val Loss: 0.15090385\n",
      "Epoch [21/100], Step [10/127], Loss: 0.01156364\n",
      "Epoch [21/100], Step [20/127], Loss: 0.01475753\n",
      "Epoch [21/100], Step [30/127], Loss: 0.00674269\n",
      "Epoch [21/100], Step [40/127], Loss: 0.00703190\n",
      "Epoch [21/100], Step [50/127], Loss: 0.00729201\n",
      "Epoch [21/100], Step [60/127], Loss: 0.00882462\n",
      "Epoch [21/100], Step [70/127], Loss: 0.00325946\n",
      "Epoch [21/100], Step [80/127], Loss: 0.00290670\n",
      "Epoch [21/100], Step [90/127], Loss: 0.00738215\n",
      "Epoch [21/100], Step [100/127], Loss: 0.00434361\n",
      "Epoch [21/100], Step [110/127], Loss: 0.00423778\n",
      "Epoch [21/100], Step [120/127], Loss: 0.00303124\n",
      "Epoch 21/100: Val Loss: 0.15475331\n",
      "Epoch [22/100], Step [10/127], Loss: 0.01157979\n",
      "Epoch [22/100], Step [20/127], Loss: 0.00838306\n",
      "Epoch [22/100], Step [30/127], Loss: 0.00307324\n",
      "Epoch [22/100], Step [40/127], Loss: 0.00446363\n",
      "Epoch [22/100], Step [50/127], Loss: 0.00469365\n",
      "Epoch [22/100], Step [60/127], Loss: 0.00522498\n",
      "Epoch [22/100], Step [70/127], Loss: 0.09424432\n",
      "Epoch [22/100], Step [80/127], Loss: 0.05616830\n",
      "Epoch [22/100], Step [90/127], Loss: 0.12278215\n",
      "Epoch [22/100], Step [100/127], Loss: 0.29423729\n",
      "Epoch [22/100], Step [110/127], Loss: 0.26015449\n",
      "Epoch [22/100], Step [120/127], Loss: 0.18609628\n",
      "Epoch 22/100: Val Loss: 0.17870987\n",
      "Epoch [23/100], Step [10/127], Loss: 1.03849709\n",
      "Epoch [23/100], Step [20/127], Loss: 0.03847335\n",
      "Epoch [23/100], Step [30/127], Loss: 1.67153358\n",
      "Epoch [23/100], Step [40/127], Loss: 2.40009499\n",
      "Epoch [23/100], Step [50/127], Loss: 0.85539705\n",
      "Epoch [23/100], Step [60/127], Loss: 0.50941163\n",
      "Epoch [23/100], Step [70/127], Loss: 0.13458385\n",
      "Epoch [23/100], Step [80/127], Loss: 3.14258242\n",
      "Epoch [23/100], Step [90/127], Loss: 0.08181730\n",
      "Epoch [23/100], Step [100/127], Loss: 0.06870572\n",
      "Epoch [23/100], Step [110/127], Loss: 0.01115477\n",
      "Epoch [23/100], Step [120/127], Loss: 0.10472902\n",
      "Epoch 23/100: Val Loss: 0.17523655\n",
      "Epoch [24/100], Step [10/127], Loss: 0.40292880\n",
      "Epoch [24/100], Step [20/127], Loss: 0.04005722\n",
      "Epoch [24/100], Step [30/127], Loss: 0.08769945\n",
      "Epoch [24/100], Step [40/127], Loss: 1.94612038\n",
      "Epoch [24/100], Step [50/127], Loss: 4.22970009\n",
      "Epoch [24/100], Step [60/127], Loss: 43.69479752\n",
      "Epoch [24/100], Step [70/127], Loss: 41.36826706\n",
      "Epoch [24/100], Step [80/127], Loss: 179.98590088\n",
      "Epoch [24/100], Step [90/127], Loss: 33.87966919\n",
      "Epoch [24/100], Step [100/127], Loss: 14.56355858\n",
      "Epoch [24/100], Step [110/127], Loss: 7.42304516\n",
      "Epoch [24/100], Step [120/127], Loss: 2.80282044\n",
      "Epoch 24/100: Val Loss: 1.26056367\n",
      "Epoch [25/100], Step [10/127], Loss: 0.48549843\n",
      "Epoch [25/100], Step [20/127], Loss: 0.60707992\n",
      "Epoch [25/100], Step [30/127], Loss: 0.16270639\n",
      "Epoch [25/100], Step [40/127], Loss: 1.01673520\n",
      "Epoch [25/100], Step [50/127], Loss: 0.24561039\n",
      "Epoch [25/100], Step [60/127], Loss: 0.23198391\n",
      "Epoch [25/100], Step [70/127], Loss: 0.11965118\n",
      "Epoch [25/100], Step [80/127], Loss: 0.05945029\n",
      "Epoch [25/100], Step [90/127], Loss: 0.02911640\n",
      "Epoch [25/100], Step [100/127], Loss: 0.03252307\n",
      "Epoch [25/100], Step [110/127], Loss: 0.01474257\n",
      "Epoch [25/100], Step [120/127], Loss: 0.01285503\n",
      "Epoch 25/100: Val Loss: 0.92452057\n",
      "Epoch [26/100], Step [10/127], Loss: 0.02578471\n",
      "Epoch [26/100], Step [20/127], Loss: 0.02093121\n",
      "Epoch [26/100], Step [30/127], Loss: 0.01620297\n",
      "Epoch [26/100], Step [40/127], Loss: 0.01598788\n",
      "Epoch [26/100], Step [50/127], Loss: 0.01189480\n",
      "Epoch [26/100], Step [60/127], Loss: 0.00806912\n",
      "Epoch [26/100], Step [70/127], Loss: 0.00650122\n",
      "Epoch [26/100], Step [80/127], Loss: 0.01267937\n",
      "Epoch [26/100], Step [90/127], Loss: 0.01077901\n",
      "Epoch [26/100], Step [100/127], Loss: 0.00882020\n",
      "Epoch [26/100], Step [110/127], Loss: 0.01389029\n",
      "Epoch [26/100], Step [120/127], Loss: 0.01196842\n",
      "Epoch 26/100: Val Loss: 3.92847398\n",
      "Epoch [27/100], Step [10/127], Loss: 0.01094059\n",
      "Epoch [27/100], Step [20/127], Loss: 0.00683964\n",
      "Epoch [27/100], Step [30/127], Loss: 0.01174598\n",
      "Epoch [27/100], Step [40/127], Loss: 0.01066104\n",
      "Epoch [27/100], Step [50/127], Loss: 0.00661287\n",
      "Epoch [27/100], Step [60/127], Loss: 0.00716013\n",
      "Epoch [27/100], Step [70/127], Loss: 0.00593077\n",
      "Epoch [27/100], Step [80/127], Loss: 0.00855654\n",
      "Epoch [27/100], Step [90/127], Loss: 0.00534122\n",
      "Epoch [27/100], Step [100/127], Loss: 0.00988045\n",
      "Epoch [27/100], Step [110/127], Loss: 0.00419436\n",
      "Epoch [27/100], Step [120/127], Loss: 0.01432483\n",
      "Epoch 27/100: Val Loss: 0.31397198\n",
      "Epoch [28/100], Step [10/127], Loss: 0.00904283\n",
      "Epoch [28/100], Step [20/127], Loss: 0.00860411\n",
      "Epoch [28/100], Step [30/127], Loss: 0.00876575\n",
      "Epoch [28/100], Step [40/127], Loss: 0.00858236\n",
      "Epoch [28/100], Step [50/127], Loss: 0.00697318\n",
      "Epoch [28/100], Step [60/127], Loss: 0.00688135\n",
      "Epoch [28/100], Step [70/127], Loss: 0.00794114\n",
      "Epoch [28/100], Step [80/127], Loss: 0.00669437\n",
      "Epoch [28/100], Step [90/127], Loss: 0.00536199\n",
      "Epoch [28/100], Step [100/127], Loss: 0.00483036\n",
      "Epoch [28/100], Step [110/127], Loss: 0.00514159\n",
      "Epoch [28/100], Step [120/127], Loss: 0.00720566\n",
      "Epoch 28/100: Val Loss: 0.22614212\n",
      "Epoch [29/100], Step [10/127], Loss: 0.00684374\n",
      "Epoch [29/100], Step [20/127], Loss: 0.00446177\n",
      "Epoch [29/100], Step [30/127], Loss: 0.00520952\n",
      "Epoch [29/100], Step [40/127], Loss: 0.00663102\n",
      "Epoch [29/100], Step [50/127], Loss: 0.00528490\n",
      "Epoch [29/100], Step [60/127], Loss: 0.00380478\n",
      "Epoch [29/100], Step [70/127], Loss: 0.00386174\n",
      "Epoch [29/100], Step [80/127], Loss: 0.00403470\n",
      "Epoch [29/100], Step [90/127], Loss: 0.00405260\n",
      "Epoch [29/100], Step [100/127], Loss: 0.00332809\n",
      "Epoch [29/100], Step [110/127], Loss: 0.00673547\n",
      "Epoch [29/100], Step [120/127], Loss: 0.00607500\n",
      "Epoch 29/100: Val Loss: 0.31096550\n",
      "Epoch [30/100], Step [10/127], Loss: 0.00724141\n",
      "Epoch [30/100], Step [20/127], Loss: 0.00555952\n",
      "Epoch [30/100], Step [30/127], Loss: 0.00829009\n",
      "Epoch [30/100], Step [40/127], Loss: 0.00548787\n",
      "Epoch [30/100], Step [50/127], Loss: 0.00289092\n",
      "Epoch [30/100], Step [60/127], Loss: 0.00365961\n",
      "Epoch [30/100], Step [70/127], Loss: 0.00338798\n",
      "Epoch [30/100], Step [80/127], Loss: 0.00381481\n",
      "Epoch [30/100], Step [90/127], Loss: 0.00567198\n",
      "Epoch [30/100], Step [100/127], Loss: 0.00347960\n",
      "Epoch [30/100], Step [110/127], Loss: 0.00508656\n",
      "Epoch [30/100], Step [120/127], Loss: 0.00386907\n",
      "Epoch 30/100: Val Loss: 0.14919340\n",
      "Epoch [31/100], Step [10/127], Loss: 0.00322265\n",
      "Epoch [31/100], Step [20/127], Loss: 0.00344813\n",
      "Epoch [31/100], Step [30/127], Loss: 0.00417808\n",
      "Epoch [31/100], Step [40/127], Loss: 0.00352745\n",
      "Epoch [31/100], Step [50/127], Loss: 0.00399640\n",
      "Epoch [31/100], Step [60/127], Loss: 0.00293246\n",
      "Epoch [31/100], Step [70/127], Loss: 0.01195041\n",
      "Epoch [31/100], Step [80/127], Loss: 0.00761629\n",
      "Epoch [31/100], Step [90/127], Loss: 0.00767414\n",
      "Epoch [31/100], Step [100/127], Loss: 0.00764445\n",
      "Epoch [31/100], Step [110/127], Loss: 0.00341492\n",
      "Epoch [31/100], Step [120/127], Loss: 0.00394461\n",
      "Epoch 31/100: Val Loss: 0.18743261\n",
      "Epoch [32/100], Step [10/127], Loss: 0.00333242\n",
      "Epoch [32/100], Step [20/127], Loss: 0.00294107\n",
      "Epoch [32/100], Step [30/127], Loss: 0.00332149\n",
      "Epoch [32/100], Step [40/127], Loss: 0.00295506\n",
      "Epoch [32/100], Step [50/127], Loss: 0.00218941\n",
      "Epoch [32/100], Step [60/127], Loss: 0.00221098\n",
      "Epoch [32/100], Step [70/127], Loss: 0.00297565\n",
      "Epoch [32/100], Step [80/127], Loss: 0.00276753\n",
      "Epoch [32/100], Step [90/127], Loss: 0.00347606\n",
      "Epoch [32/100], Step [100/127], Loss: 0.00171940\n",
      "Epoch [32/100], Step [110/127], Loss: 0.00354063\n",
      "Epoch [32/100], Step [120/127], Loss: 0.00452979\n",
      "Epoch 32/100: Val Loss: 0.30400652\n",
      "Epoch [33/100], Step [10/127], Loss: 0.00312385\n",
      "Epoch [33/100], Step [20/127], Loss: 0.00251732\n",
      "Epoch [33/100], Step [30/127], Loss: 0.00281690\n",
      "Epoch [33/100], Step [40/127], Loss: 0.00305235\n",
      "Epoch [33/100], Step [50/127], Loss: 0.01106379\n",
      "Epoch [33/100], Step [60/127], Loss: 0.00712762\n",
      "Epoch [33/100], Step [70/127], Loss: 0.00392025\n",
      "Epoch [33/100], Step [80/127], Loss: 0.00298629\n",
      "Epoch [33/100], Step [90/127], Loss: 0.00623436\n",
      "Epoch [33/100], Step [100/127], Loss: 0.00386008\n",
      "Epoch [33/100], Step [110/127], Loss: 0.00154044\n",
      "Epoch [33/100], Step [120/127], Loss: 0.00258159\n",
      "Epoch 33/100: Val Loss: 0.18969960\n",
      "Epoch [34/100], Step [10/127], Loss: 0.00184971\n",
      "Epoch [34/100], Step [20/127], Loss: 0.00207104\n",
      "Epoch [34/100], Step [30/127], Loss: 0.00199185\n",
      "Epoch [34/100], Step [40/127], Loss: 0.00205535\n",
      "Epoch [34/100], Step [50/127], Loss: 0.00252474\n",
      "Epoch [34/100], Step [60/127], Loss: 0.00193769\n",
      "Epoch [34/100], Step [70/127], Loss: 0.00187539\n",
      "Epoch [34/100], Step [80/127], Loss: 0.00640997\n",
      "Epoch [34/100], Step [90/127], Loss: 0.00190427\n",
      "Epoch [34/100], Step [100/127], Loss: 0.00198439\n",
      "Epoch [34/100], Step [110/127], Loss: 0.00187509\n",
      "Epoch [34/100], Step [120/127], Loss: 0.00155957\n",
      "Epoch 34/100: Val Loss: 0.15432404\n",
      "Epoch [35/100], Step [10/127], Loss: 0.00269800\n",
      "Epoch [35/100], Step [20/127], Loss: 0.00245980\n",
      "Epoch [35/100], Step [30/127], Loss: 0.00153486\n",
      "Epoch [35/100], Step [40/127], Loss: 0.00307198\n",
      "Epoch [35/100], Step [50/127], Loss: 0.00196327\n",
      "Epoch [35/100], Step [60/127], Loss: 0.00267626\n",
      "Epoch [35/100], Step [70/127], Loss: 0.00157549\n",
      "Epoch [35/100], Step [80/127], Loss: 0.00244844\n",
      "Epoch [35/100], Step [90/127], Loss: 0.00232477\n",
      "Epoch [35/100], Step [100/127], Loss: 0.00136496\n",
      "Epoch [35/100], Step [110/127], Loss: 0.00182977\n",
      "Epoch [35/100], Step [120/127], Loss: 0.00256445\n",
      "Epoch 35/100: Val Loss: 0.89524407\n",
      "Epoch [36/100], Step [10/127], Loss: 0.00282692\n",
      "Epoch [36/100], Step [20/127], Loss: 0.00343723\n",
      "Epoch [36/100], Step [30/127], Loss: 0.00386623\n",
      "Epoch [36/100], Step [40/127], Loss: 0.00505523\n",
      "Epoch [36/100], Step [50/127], Loss: 0.00178530\n",
      "Epoch [36/100], Step [60/127], Loss: 0.00293778\n",
      "Epoch [36/100], Step [70/127], Loss: 0.00309701\n",
      "Epoch [36/100], Step [80/127], Loss: 0.00217222\n",
      "Epoch [36/100], Step [90/127], Loss: 0.00196545\n",
      "Epoch [36/100], Step [100/127], Loss: 0.00339837\n",
      "Epoch [36/100], Step [110/127], Loss: 0.00539935\n",
      "Epoch [36/100], Step [120/127], Loss: 0.00176317\n",
      "Epoch 36/100: Val Loss: 0.31408431\n",
      "Epoch [37/100], Step [10/127], Loss: 0.00100802\n",
      "Epoch [37/100], Step [20/127], Loss: 0.00124413\n",
      "Epoch [37/100], Step [30/127], Loss: 0.00520666\n",
      "Epoch [37/100], Step [40/127], Loss: 0.00272632\n",
      "Epoch [37/100], Step [50/127], Loss: 0.00426788\n",
      "Epoch [37/100], Step [60/127], Loss: 0.00193873\n",
      "Epoch [37/100], Step [70/127], Loss: 0.00382972\n",
      "Epoch [37/100], Step [80/127], Loss: 0.00408082\n",
      "Epoch [37/100], Step [90/127], Loss: 0.00159955\n",
      "Epoch [37/100], Step [100/127], Loss: 0.00129517\n",
      "Epoch [37/100], Step [110/127], Loss: 0.00372210\n",
      "Epoch [37/100], Step [120/127], Loss: 0.00293511\n",
      "Epoch 37/100: Val Loss: 0.81718782\n",
      "Epoch [38/100], Step [10/127], Loss: 0.00662853\n",
      "Epoch [38/100], Step [20/127], Loss: 0.00252819\n",
      "Epoch [38/100], Step [30/127], Loss: 0.00267978\n",
      "Epoch [38/100], Step [40/127], Loss: 0.00211683\n",
      "Epoch [38/100], Step [50/127], Loss: 0.00197495\n",
      "Epoch [38/100], Step [60/127], Loss: 0.00115000\n",
      "Epoch [38/100], Step [70/127], Loss: 0.00116105\n",
      "Epoch [38/100], Step [80/127], Loss: 0.00127352\n",
      "Epoch [38/100], Step [90/127], Loss: 0.00112896\n",
      "Epoch [38/100], Step [100/127], Loss: 0.00178826\n",
      "Epoch [38/100], Step [110/127], Loss: 0.00120606\n",
      "Epoch [38/100], Step [120/127], Loss: 0.00112635\n",
      "Epoch 38/100: Val Loss: 0.55377282\n",
      "Epoch [39/100], Step [10/127], Loss: 0.00153563\n",
      "Epoch [39/100], Step [20/127], Loss: 0.00148338\n",
      "Epoch [39/100], Step [30/127], Loss: 0.00125223\n",
      "Epoch [39/100], Step [40/127], Loss: 0.00233524\n",
      "Epoch [39/100], Step [50/127], Loss: 0.00460319\n",
      "Epoch [39/100], Step [60/127], Loss: 0.00585689\n",
      "Epoch [39/100], Step [70/127], Loss: 0.00144581\n",
      "Epoch [39/100], Step [80/127], Loss: 0.00486259\n",
      "Epoch [39/100], Step [90/127], Loss: 0.00303245\n",
      "Epoch [39/100], Step [100/127], Loss: 0.00155828\n",
      "Epoch [39/100], Step [110/127], Loss: 0.00250637\n",
      "Epoch [39/100], Step [120/127], Loss: 0.00492886\n",
      "Epoch 39/100: Val Loss: 0.27088868\n",
      "Epoch [40/100], Step [10/127], Loss: 0.00954823\n",
      "Epoch [40/100], Step [20/127], Loss: 0.00382797\n",
      "Epoch [40/100], Step [30/127], Loss: 0.00165900\n",
      "Epoch [40/100], Step [40/127], Loss: 0.00151704\n",
      "Epoch [40/100], Step [50/127], Loss: 0.00158227\n",
      "Epoch [40/100], Step [60/127], Loss: 0.00240129\n",
      "Epoch [40/100], Step [70/127], Loss: 0.02632821\n",
      "Epoch [40/100], Step [80/127], Loss: 0.02733300\n",
      "Epoch [40/100], Step [90/127], Loss: 0.00379733\n",
      "Epoch [40/100], Step [100/127], Loss: 0.05623087\n",
      "Epoch [40/100], Step [110/127], Loss: 0.03159979\n",
      "Epoch [40/100], Step [120/127], Loss: 0.11470157\n",
      "Epoch 40/100: Val Loss: 0.19175293\n",
      "Epoch [41/100], Step [10/127], Loss: 0.03572300\n",
      "Epoch [41/100], Step [20/127], Loss: 0.02883609\n",
      "Epoch [41/100], Step [30/127], Loss: 0.00479604\n",
      "Epoch [41/100], Step [40/127], Loss: 0.01216287\n",
      "Epoch [41/100], Step [50/127], Loss: 0.00328871\n",
      "Epoch [41/100], Step [60/127], Loss: 0.00300329\n",
      "Epoch [41/100], Step [70/127], Loss: 0.00261188\n",
      "Epoch [41/100], Step [80/127], Loss: 0.00316117\n",
      "Epoch [41/100], Step [90/127], Loss: 0.00439605\n",
      "Epoch [41/100], Step [100/127], Loss: 0.01903073\n",
      "Epoch [41/100], Step [110/127], Loss: 0.17661203\n",
      "Epoch [41/100], Step [120/127], Loss: 0.69969600\n",
      "Epoch 41/100: Val Loss: 0.20490054\n",
      "Epoch [42/100], Step [10/127], Loss: 3.04642558\n",
      "Epoch [42/100], Step [20/127], Loss: 8.99584103\n",
      "Epoch [42/100], Step [30/127], Loss: 12.48748970\n",
      "Epoch [42/100], Step [40/127], Loss: 0.10160602\n",
      "Epoch [42/100], Step [50/127], Loss: 0.18288575\n",
      "Epoch [42/100], Step [60/127], Loss: 0.06973403\n",
      "Epoch [42/100], Step [70/127], Loss: 0.01061620\n",
      "Epoch [42/100], Step [80/127], Loss: 0.02517677\n",
      "Epoch [42/100], Step [90/127], Loss: 0.06040300\n",
      "Epoch [42/100], Step [100/127], Loss: 0.10964803\n",
      "Epoch [42/100], Step [110/127], Loss: 0.21008702\n",
      "Epoch [42/100], Step [120/127], Loss: 0.03702622\n",
      "Epoch 42/100: Val Loss: 0.27953052\n",
      "Epoch [43/100], Step [10/127], Loss: 0.00374655\n",
      "Epoch [43/100], Step [20/127], Loss: 0.00181198\n",
      "Epoch [43/100], Step [30/127], Loss: 0.00419711\n",
      "Epoch [43/100], Step [40/127], Loss: 0.00502989\n",
      "Epoch [43/100], Step [50/127], Loss: 0.00157853\n",
      "Epoch [43/100], Step [60/127], Loss: 0.00373081\n",
      "Epoch [43/100], Step [70/127], Loss: 0.00184611\n",
      "Epoch [43/100], Step [80/127], Loss: 0.00190670\n",
      "Epoch [43/100], Step [90/127], Loss: 0.00419500\n",
      "Epoch [43/100], Step [100/127], Loss: 0.00236416\n",
      "Epoch [43/100], Step [110/127], Loss: 0.00396373\n",
      "Epoch [43/100], Step [120/127], Loss: 0.00234972\n",
      "Epoch 43/100: Val Loss: 0.28864337\n",
      "Epoch [44/100], Step [10/127], Loss: 0.00218849\n",
      "Epoch [44/100], Step [20/127], Loss: 0.00094401\n",
      "Epoch [44/100], Step [30/127], Loss: 0.00325178\n",
      "Epoch [44/100], Step [40/127], Loss: 0.00421898\n",
      "Epoch [44/100], Step [50/127], Loss: 0.00280112\n",
      "Epoch [44/100], Step [60/127], Loss: 0.00230776\n",
      "Epoch [44/100], Step [70/127], Loss: 0.00120828\n",
      "Epoch [44/100], Step [80/127], Loss: 0.00132765\n",
      "Epoch [44/100], Step [90/127], Loss: 0.00146846\n",
      "Epoch [44/100], Step [100/127], Loss: 0.00123073\n",
      "Epoch [44/100], Step [110/127], Loss: 0.00498824\n",
      "Epoch [44/100], Step [120/127], Loss: 0.02445957\n",
      "Epoch 44/100: Val Loss: 0.38214964\n",
      "Epoch [45/100], Step [10/127], Loss: 0.00170776\n",
      "Epoch [45/100], Step [20/127], Loss: 0.02009164\n",
      "Epoch [45/100], Step [30/127], Loss: 0.00991947\n",
      "Epoch [45/100], Step [40/127], Loss: 0.00929264\n",
      "Epoch [45/100], Step [50/127], Loss: 0.00967901\n",
      "Epoch [45/100], Step [60/127], Loss: 0.00317319\n",
      "Epoch [45/100], Step [70/127], Loss: 0.00160158\n",
      "Epoch [45/100], Step [80/127], Loss: 0.00431608\n",
      "Epoch [45/100], Step [90/127], Loss: 0.00312661\n",
      "Epoch [45/100], Step [100/127], Loss: 0.00132315\n",
      "Epoch [45/100], Step [110/127], Loss: 0.00518657\n",
      "Epoch [45/100], Step [120/127], Loss: 0.00185415\n",
      "Epoch 45/100: Val Loss: 0.51603112\n",
      "Epoch [46/100], Step [10/127], Loss: 0.00155368\n",
      "Epoch [46/100], Step [20/127], Loss: 0.00263919\n",
      "Epoch [46/100], Step [30/127], Loss: 0.00190941\n",
      "Epoch [46/100], Step [40/127], Loss: 0.00183778\n",
      "Epoch [46/100], Step [50/127], Loss: 0.01007260\n",
      "Epoch [46/100], Step [60/127], Loss: 0.00963753\n",
      "Epoch [46/100], Step [70/127], Loss: 0.03220413\n",
      "Epoch [46/100], Step [80/127], Loss: 0.00825514\n",
      "Epoch [46/100], Step [90/127], Loss: 0.01148440\n",
      "Epoch [46/100], Step [100/127], Loss: 0.00395804\n",
      "Epoch [46/100], Step [110/127], Loss: 0.02337084\n",
      "Epoch [46/100], Step [120/127], Loss: 0.01878435\n",
      "Epoch 46/100: Val Loss: 0.38118178\n",
      "Epoch [47/100], Step [10/127], Loss: 0.00448951\n",
      "Epoch [47/100], Step [20/127], Loss: 0.01529775\n",
      "Epoch [47/100], Step [30/127], Loss: 0.00259009\n",
      "Epoch [47/100], Step [40/127], Loss: 0.00913355\n",
      "Epoch [47/100], Step [50/127], Loss: 0.00295516\n",
      "Epoch [47/100], Step [60/127], Loss: 0.00454740\n",
      "Epoch [47/100], Step [70/127], Loss: 0.01121014\n",
      "Epoch [47/100], Step [80/127], Loss: 0.00361208\n",
      "Epoch [47/100], Step [90/127], Loss: 0.03731183\n",
      "Epoch [47/100], Step [100/127], Loss: 0.28734428\n",
      "Epoch [47/100], Step [110/127], Loss: 2.06285262\n",
      "Epoch [47/100], Step [120/127], Loss: 1.30403829\n",
      "Epoch 47/100: Val Loss: 0.28580912\n",
      "Epoch [48/100], Step [10/127], Loss: 0.00717303\n",
      "Epoch [48/100], Step [20/127], Loss: 0.15635644\n",
      "Epoch [48/100], Step [30/127], Loss: 0.00425501\n",
      "Epoch [48/100], Step [40/127], Loss: 0.01841732\n",
      "Epoch [48/100], Step [50/127], Loss: 0.00553847\n",
      "Epoch [48/100], Step [60/127], Loss: 0.00829191\n",
      "Epoch [48/100], Step [70/127], Loss: 0.00851748\n",
      "Epoch [48/100], Step [80/127], Loss: 0.00316151\n",
      "Epoch [48/100], Step [90/127], Loss: 0.00393425\n",
      "Epoch [48/100], Step [100/127], Loss: 0.00248951\n",
      "Epoch [48/100], Step [110/127], Loss: 0.00325126\n",
      "Epoch [48/100], Step [120/127], Loss: 0.00159769\n",
      "Epoch 48/100: Val Loss: 0.29099166\n",
      "Epoch [49/100], Step [10/127], Loss: 0.00124417\n",
      "Epoch [49/100], Step [20/127], Loss: 0.00156194\n",
      "Epoch [49/100], Step [30/127], Loss: 0.00165980\n",
      "Epoch [49/100], Step [40/127], Loss: 0.00393890\n",
      "Epoch [49/100], Step [50/127], Loss: 0.00966248\n",
      "Epoch [49/100], Step [60/127], Loss: 0.00561384\n",
      "Epoch [49/100], Step [70/127], Loss: 0.01203944\n",
      "Epoch [49/100], Step [80/127], Loss: 0.00827706\n",
      "Epoch [49/100], Step [90/127], Loss: 0.01954198\n",
      "Epoch [49/100], Step [100/127], Loss: 0.00208786\n",
      "Epoch [49/100], Step [110/127], Loss: 0.00233547\n",
      "Epoch [49/100], Step [120/127], Loss: 0.00146345\n",
      "Epoch 49/100: Val Loss: 0.24558345\n",
      "Epoch [50/100], Step [10/127], Loss: 0.00177591\n",
      "Epoch [50/100], Step [20/127], Loss: 0.00403563\n",
      "Epoch [50/100], Step [30/127], Loss: 0.00385228\n",
      "Epoch [50/100], Step [40/127], Loss: 0.09002663\n",
      "Epoch [50/100], Step [50/127], Loss: 0.01421829\n",
      "Epoch [50/100], Step [60/127], Loss: 0.00466984\n",
      "Epoch [50/100], Step [70/127], Loss: 0.00439572\n",
      "Epoch [50/100], Step [80/127], Loss: 0.00294169\n",
      "Epoch [50/100], Step [90/127], Loss: 0.00132240\n",
      "Epoch [50/100], Step [100/127], Loss: 0.00632529\n",
      "Epoch [50/100], Step [110/127], Loss: 0.00452425\n",
      "Epoch [50/100], Step [120/127], Loss: 0.00468899\n",
      "Epoch 50/100: Val Loss: 0.26352503\n",
      "Epoch [51/100], Step [10/127], Loss: 0.06479475\n",
      "Epoch [51/100], Step [20/127], Loss: 0.02995317\n",
      "Epoch [51/100], Step [30/127], Loss: 0.03158930\n",
      "Epoch [51/100], Step [40/127], Loss: 0.00779322\n",
      "Epoch [51/100], Step [50/127], Loss: 0.24343206\n",
      "Epoch [51/100], Step [60/127], Loss: 0.49465469\n",
      "Epoch [51/100], Step [70/127], Loss: 0.12619537\n",
      "Epoch [51/100], Step [80/127], Loss: 0.04293492\n",
      "Epoch [51/100], Step [90/127], Loss: 1.61105096\n",
      "Epoch [51/100], Step [100/127], Loss: 0.34619474\n",
      "Epoch [51/100], Step [110/127], Loss: 0.74810392\n",
      "Epoch [51/100], Step [120/127], Loss: 0.03435152\n",
      "Epoch 51/100: Val Loss: 0.36545843\n",
      "Epoch [52/100], Step [10/127], Loss: 3.62766433\n",
      "Epoch [52/100], Step [20/127], Loss: 12.64155197\n",
      "Epoch [52/100], Step [30/127], Loss: 0.11639847\n",
      "Epoch [52/100], Step [40/127], Loss: 0.01175253\n",
      "Epoch [52/100], Step [50/127], Loss: 0.07511865\n",
      "Epoch [52/100], Step [60/127], Loss: 0.02050595\n",
      "Epoch [52/100], Step [70/127], Loss: 0.04833804\n",
      "Epoch [52/100], Step [80/127], Loss: 0.04418102\n",
      "Epoch [52/100], Step [90/127], Loss: 0.01869148\n",
      "Epoch [52/100], Step [100/127], Loss: 0.00525074\n",
      "Epoch [52/100], Step [110/127], Loss: 0.00545065\n",
      "Epoch [52/100], Step [120/127], Loss: 0.01165896\n",
      "Epoch 52/100: Val Loss: 0.65063455\n",
      "Epoch [53/100], Step [10/127], Loss: 0.24364580\n",
      "Epoch [53/100], Step [20/127], Loss: 0.03161413\n",
      "Epoch [53/100], Step [30/127], Loss: 0.05934432\n",
      "Epoch [53/100], Step [40/127], Loss: 0.02569104\n",
      "Epoch [53/100], Step [50/127], Loss: 0.00967406\n",
      "Epoch [53/100], Step [60/127], Loss: 0.01915877\n",
      "Epoch [53/100], Step [70/127], Loss: 0.03388124\n",
      "Epoch [53/100], Step [80/127], Loss: 0.04765584\n",
      "Epoch [53/100], Step [90/127], Loss: 0.11704652\n",
      "Epoch [53/100], Step [100/127], Loss: 0.21997711\n",
      "Epoch [53/100], Step [110/127], Loss: 0.03799717\n",
      "Epoch [53/100], Step [120/127], Loss: 0.04834007\n",
      "Epoch 53/100: Val Loss: 1.07354739\n",
      "Epoch [54/100], Step [10/127], Loss: 0.26962098\n",
      "Epoch [54/100], Step [20/127], Loss: 0.06306851\n",
      "Epoch [54/100], Step [30/127], Loss: 0.02361991\n",
      "Epoch [54/100], Step [40/127], Loss: 2.05127025\n",
      "Epoch [54/100], Step [50/127], Loss: 0.92298836\n",
      "Epoch [54/100], Step [60/127], Loss: 1.09816384\n",
      "Epoch [54/100], Step [70/127], Loss: 0.02967498\n",
      "Epoch [54/100], Step [80/127], Loss: 0.00459407\n",
      "Epoch [54/100], Step [90/127], Loss: 0.00171751\n",
      "Epoch [54/100], Step [100/127], Loss: 0.01397241\n",
      "Epoch [54/100], Step [110/127], Loss: 0.01944945\n",
      "Epoch [54/100], Step [120/127], Loss: 0.08811107\n",
      "Epoch 54/100: Val Loss: 0.50243233\n",
      "Epoch [55/100], Step [10/127], Loss: 0.01708992\n",
      "Epoch [55/100], Step [20/127], Loss: 0.06867830\n",
      "Epoch [55/100], Step [30/127], Loss: 0.09647167\n",
      "Epoch [55/100], Step [40/127], Loss: 0.01186906\n",
      "Epoch [55/100], Step [50/127], Loss: 0.00546871\n",
      "Epoch [55/100], Step [60/127], Loss: 0.00518931\n",
      "Epoch [55/100], Step [70/127], Loss: 0.00947506\n",
      "Epoch [55/100], Step [80/127], Loss: 0.00477874\n",
      "Epoch [55/100], Step [90/127], Loss: 0.01074766\n",
      "Epoch [55/100], Step [100/127], Loss: 0.02567992\n",
      "Epoch [55/100], Step [110/127], Loss: 0.01220439\n",
      "Epoch [55/100], Step [120/127], Loss: 0.00569874\n",
      "Epoch 55/100: Val Loss: 0.19259686\n",
      "Epoch [56/100], Step [10/127], Loss: 0.00220559\n",
      "Epoch [56/100], Step [20/127], Loss: 0.01857006\n",
      "Epoch [56/100], Step [30/127], Loss: 0.00510687\n",
      "Epoch [56/100], Step [40/127], Loss: 0.00380618\n",
      "Epoch [56/100], Step [50/127], Loss: 0.00284263\n",
      "Epoch [56/100], Step [60/127], Loss: 0.02839151\n",
      "Epoch [56/100], Step [70/127], Loss: 0.00659494\n",
      "Epoch [56/100], Step [80/127], Loss: 0.00840853\n",
      "Epoch [56/100], Step [90/127], Loss: 0.11213802\n",
      "Epoch [56/100], Step [100/127], Loss: 0.01691642\n",
      "Epoch [56/100], Step [110/127], Loss: 0.52389795\n",
      "Epoch [56/100], Step [120/127], Loss: 0.20455265\n",
      "Epoch 56/100: Val Loss: 0.48583301\n",
      "Epoch [57/100], Step [10/127], Loss: 0.85181779\n",
      "Epoch [57/100], Step [20/127], Loss: 3.06695724\n",
      "Epoch [57/100], Step [30/127], Loss: 2.98667312\n",
      "Epoch [57/100], Step [40/127], Loss: 2.67628360\n",
      "Epoch [57/100], Step [50/127], Loss: 1.42288303\n",
      "Epoch [57/100], Step [60/127], Loss: 0.39478356\n",
      "Epoch [57/100], Step [70/127], Loss: 0.07493351\n",
      "Epoch [57/100], Step [80/127], Loss: 1.68465543\n",
      "Epoch [57/100], Step [90/127], Loss: 0.75296187\n",
      "Epoch [57/100], Step [100/127], Loss: 1.77331614\n",
      "Epoch [57/100], Step [110/127], Loss: 0.63419515\n",
      "Epoch [57/100], Step [120/127], Loss: 8.90676022\n",
      "Epoch 57/100: Val Loss: 0.51722037\n",
      "Epoch [58/100], Step [10/127], Loss: 0.94195741\n",
      "Epoch [58/100], Step [20/127], Loss: 0.66082376\n",
      "Epoch [58/100], Step [30/127], Loss: 0.48199791\n",
      "Epoch [58/100], Step [40/127], Loss: 0.03936412\n",
      "Epoch [58/100], Step [50/127], Loss: 0.96277559\n",
      "Epoch [58/100], Step [60/127], Loss: 0.23750325\n",
      "Epoch [58/100], Step [70/127], Loss: 0.15485683\n",
      "Epoch [58/100], Step [80/127], Loss: 0.03928420\n",
      "Epoch [58/100], Step [90/127], Loss: 0.01693833\n",
      "Epoch [58/100], Step [100/127], Loss: 0.01427232\n",
      "Epoch [58/100], Step [110/127], Loss: 0.02536190\n",
      "Epoch [58/100], Step [120/127], Loss: 0.01534188\n",
      "Epoch 58/100: Val Loss: 0.50380887\n",
      "Epoch [59/100], Step [10/127], Loss: 0.00436868\n",
      "Epoch [59/100], Step [20/127], Loss: 0.00770190\n",
      "Epoch [59/100], Step [30/127], Loss: 0.02277727\n",
      "Epoch [59/100], Step [40/127], Loss: 0.03814369\n",
      "Epoch [59/100], Step [50/127], Loss: 0.00697207\n",
      "Epoch [59/100], Step [60/127], Loss: 0.00442814\n",
      "Epoch [59/100], Step [70/127], Loss: 0.00287485\n",
      "Epoch [59/100], Step [80/127], Loss: 0.00280022\n",
      "Epoch [59/100], Step [90/127], Loss: 0.00201995\n",
      "Epoch [59/100], Step [100/127], Loss: 0.00273569\n",
      "Epoch [59/100], Step [110/127], Loss: 0.00284047\n",
      "Epoch [59/100], Step [120/127], Loss: 0.00193689\n",
      "Epoch 59/100: Val Loss: 0.19802066\n",
      "Epoch [60/100], Step [10/127], Loss: 0.00340361\n",
      "Epoch [60/100], Step [20/127], Loss: 0.00225578\n",
      "Epoch [60/100], Step [30/127], Loss: 0.00337221\n",
      "Epoch [60/100], Step [40/127], Loss: 0.00393646\n",
      "Epoch [60/100], Step [50/127], Loss: 0.00247865\n",
      "Epoch [60/100], Step [60/127], Loss: 0.00741253\n",
      "Epoch [60/100], Step [70/127], Loss: 0.00217815\n",
      "Epoch [60/100], Step [80/127], Loss: 0.00217403\n",
      "Epoch [60/100], Step [90/127], Loss: 0.00176988\n",
      "Epoch [60/100], Step [100/127], Loss: 0.00798675\n",
      "Epoch [60/100], Step [110/127], Loss: 0.01219445\n",
      "Epoch [60/100], Step [120/127], Loss: 0.00362812\n",
      "Epoch 60/100: Val Loss: 0.26238652\n",
      "Epoch [61/100], Step [10/127], Loss: 0.06772768\n",
      "Epoch [61/100], Step [20/127], Loss: 0.00126352\n",
      "Epoch [61/100], Step [30/127], Loss: 0.00238040\n",
      "Epoch [61/100], Step [40/127], Loss: 0.01505644\n",
      "Epoch [61/100], Step [50/127], Loss: 0.00788566\n",
      "Epoch [61/100], Step [60/127], Loss: 0.00231273\n",
      "Epoch [61/100], Step [70/127], Loss: 0.00352857\n",
      "Epoch [61/100], Step [80/127], Loss: 0.00179306\n",
      "Epoch [61/100], Step [90/127], Loss: 0.00131049\n",
      "Epoch [61/100], Step [100/127], Loss: 0.00365018\n",
      "Epoch [61/100], Step [110/127], Loss: 0.00543946\n",
      "Epoch [61/100], Step [120/127], Loss: 0.00155006\n",
      "Epoch 61/100: Val Loss: 1.03546388\n",
      "Epoch [62/100], Step [10/127], Loss: 0.00369837\n",
      "Epoch [62/100], Step [20/127], Loss: 0.00157429\n",
      "Epoch [62/100], Step [30/127], Loss: 0.00125867\n",
      "Epoch [62/100], Step [40/127], Loss: 0.00521571\n",
      "Epoch [62/100], Step [50/127], Loss: 0.00870081\n",
      "Epoch [62/100], Step [60/127], Loss: 0.00412626\n",
      "Epoch [62/100], Step [70/127], Loss: 0.00239358\n",
      "Epoch [62/100], Step [80/127], Loss: 0.00128857\n",
      "Epoch [62/100], Step [90/127], Loss: 0.00164690\n",
      "Epoch [62/100], Step [100/127], Loss: 0.00572361\n",
      "Epoch [62/100], Step [110/127], Loss: 0.00522573\n",
      "Epoch [62/100], Step [120/127], Loss: 0.00389002\n",
      "Epoch 62/100: Val Loss: 0.42386865\n",
      "Epoch [63/100], Step [10/127], Loss: 0.00232598\n",
      "Epoch [63/100], Step [20/127], Loss: 0.00272049\n",
      "Epoch [63/100], Step [30/127], Loss: 0.00258182\n",
      "Epoch [63/100], Step [40/127], Loss: 0.00451371\n",
      "Epoch [63/100], Step [50/127], Loss: 0.01003601\n",
      "Epoch [63/100], Step [60/127], Loss: 0.00215270\n",
      "Epoch [63/100], Step [70/127], Loss: 0.02517246\n",
      "Epoch [63/100], Step [80/127], Loss: 0.00263685\n",
      "Epoch [63/100], Step [90/127], Loss: 0.00511347\n",
      "Epoch [63/100], Step [100/127], Loss: 0.01066915\n",
      "Epoch [63/100], Step [110/127], Loss: 0.00447021\n",
      "Epoch [63/100], Step [120/127], Loss: 0.00137724\n",
      "Epoch 63/100: Val Loss: 0.44728079\n",
      "Epoch [64/100], Step [10/127], Loss: 0.00269364\n",
      "Epoch [64/100], Step [20/127], Loss: 0.00545279\n",
      "Epoch [64/100], Step [30/127], Loss: 0.00972232\n",
      "Epoch [64/100], Step [40/127], Loss: 0.00317599\n",
      "Epoch [64/100], Step [50/127], Loss: 0.01123345\n",
      "Epoch [64/100], Step [60/127], Loss: 0.00214242\n",
      "Epoch [64/100], Step [70/127], Loss: 0.00176144\n",
      "Epoch [64/100], Step [80/127], Loss: 0.00109858\n",
      "Epoch [64/100], Step [90/127], Loss: 0.00434281\n",
      "Epoch [64/100], Step [100/127], Loss: 0.00544979\n",
      "Epoch [64/100], Step [110/127], Loss: 0.01377223\n",
      "Epoch [64/100], Step [120/127], Loss: 0.00908358\n",
      "Epoch 64/100: Val Loss: 0.23145628\n",
      "Epoch [65/100], Step [10/127], Loss: 0.02064358\n",
      "Epoch [65/100], Step [20/127], Loss: 0.03378064\n",
      "Epoch [65/100], Step [30/127], Loss: 0.00406208\n",
      "Epoch [65/100], Step [40/127], Loss: 0.00677252\n",
      "Epoch [65/100], Step [50/127], Loss: 0.00501802\n",
      "Epoch [65/100], Step [60/127], Loss: 0.00976509\n",
      "Epoch [65/100], Step [70/127], Loss: 0.00527999\n",
      "Epoch [65/100], Step [80/127], Loss: 0.15998055\n",
      "Epoch [65/100], Step [90/127], Loss: 0.00758799\n",
      "Epoch [65/100], Step [100/127], Loss: 0.47447428\n",
      "Epoch [65/100], Step [110/127], Loss: 0.25885344\n",
      "Epoch [65/100], Step [120/127], Loss: 0.01483247\n",
      "Epoch 65/100: Val Loss: 0.21712968\n",
      "Epoch [66/100], Step [10/127], Loss: 0.46543279\n",
      "Epoch [66/100], Step [20/127], Loss: 0.24527469\n",
      "Epoch [66/100], Step [30/127], Loss: 1.46640694\n",
      "Epoch [66/100], Step [40/127], Loss: 0.40807575\n",
      "Epoch [66/100], Step [50/127], Loss: 0.24426274\n",
      "Epoch [66/100], Step [60/127], Loss: 0.15239677\n",
      "Epoch [66/100], Step [70/127], Loss: 0.24422617\n",
      "Epoch [66/100], Step [80/127], Loss: 0.04395392\n",
      "Epoch [66/100], Step [90/127], Loss: 0.21604319\n",
      "Epoch [66/100], Step [100/127], Loss: 0.09782187\n",
      "Epoch [66/100], Step [110/127], Loss: 0.06398184\n",
      "Epoch [66/100], Step [120/127], Loss: 0.17784970\n",
      "Epoch 66/100: Val Loss: 0.27416896\n",
      "Epoch [67/100], Step [10/127], Loss: 0.08393798\n",
      "Epoch [67/100], Step [20/127], Loss: 1.17373407\n",
      "Epoch [67/100], Step [30/127], Loss: 0.08792669\n",
      "Epoch [67/100], Step [40/127], Loss: 0.12571080\n",
      "Epoch [67/100], Step [50/127], Loss: 0.32530025\n",
      "Epoch [67/100], Step [60/127], Loss: 9.11490536\n",
      "Epoch [67/100], Step [70/127], Loss: 4.05175543\n",
      "Epoch [67/100], Step [80/127], Loss: 0.90508342\n",
      "Epoch [67/100], Step [90/127], Loss: 0.55726153\n",
      "Epoch [67/100], Step [100/127], Loss: 0.18456723\n",
      "Epoch [67/100], Step [110/127], Loss: 0.03488820\n",
      "Epoch [67/100], Step [120/127], Loss: 0.12544356\n",
      "Epoch 67/100: Val Loss: 0.25601909\n",
      "Epoch [68/100], Step [10/127], Loss: 0.04551781\n",
      "Epoch [68/100], Step [20/127], Loss: 0.09911469\n",
      "Epoch [68/100], Step [30/127], Loss: 0.21758169\n",
      "Epoch [68/100], Step [40/127], Loss: 0.15323307\n",
      "Epoch [68/100], Step [50/127], Loss: 0.22458789\n",
      "Epoch [68/100], Step [60/127], Loss: 0.21212733\n",
      "Epoch [68/100], Step [70/127], Loss: 0.38554341\n",
      "Epoch [68/100], Step [80/127], Loss: 0.07070996\n",
      "Epoch [68/100], Step [90/127], Loss: 0.02922278\n",
      "Epoch [68/100], Step [100/127], Loss: 0.05349093\n",
      "Epoch [68/100], Step [110/127], Loss: 0.01297696\n",
      "Epoch [68/100], Step [120/127], Loss: 0.01907787\n",
      "Epoch 68/100: Val Loss: 0.23233240\n",
      "Epoch [69/100], Step [10/127], Loss: 0.00225447\n",
      "Epoch [69/100], Step [20/127], Loss: 0.00664965\n",
      "Epoch [69/100], Step [30/127], Loss: 0.00371062\n",
      "Epoch [69/100], Step [40/127], Loss: 0.00201303\n",
      "Epoch [69/100], Step [50/127], Loss: 0.00565162\n",
      "Epoch [69/100], Step [60/127], Loss: 0.00506558\n",
      "Epoch [69/100], Step [70/127], Loss: 0.00354510\n",
      "Epoch [69/100], Step [80/127], Loss: 0.00550225\n",
      "Epoch [69/100], Step [90/127], Loss: 0.00513293\n",
      "Epoch [69/100], Step [100/127], Loss: 0.00224455\n",
      "Epoch [69/100], Step [110/127], Loss: 0.00207677\n",
      "Epoch [69/100], Step [120/127], Loss: 0.00177578\n",
      "Epoch 69/100: Val Loss: 5.21286864\n",
      "Epoch [70/100], Step [10/127], Loss: 0.00354067\n",
      "Epoch [70/100], Step [20/127], Loss: 0.00318660\n",
      "Epoch [70/100], Step [30/127], Loss: 0.00134287\n",
      "Epoch [70/100], Step [40/127], Loss: 0.00601087\n",
      "Epoch [70/100], Step [50/127], Loss: 0.00202737\n",
      "Epoch [70/100], Step [60/127], Loss: 0.00168110\n",
      "Epoch [70/100], Step [70/127], Loss: 0.00689820\n",
      "Epoch [70/100], Step [80/127], Loss: 0.01155716\n",
      "Epoch [70/100], Step [90/127], Loss: 0.00251390\n",
      "Epoch [70/100], Step [100/127], Loss: 0.00400012\n",
      "Epoch [70/100], Step [110/127], Loss: 0.00629234\n",
      "Epoch [70/100], Step [120/127], Loss: 0.00433178\n",
      "Epoch 70/100: Val Loss: 1.46337890\n",
      "Epoch [71/100], Step [10/127], Loss: 0.00204926\n",
      "Epoch [71/100], Step [20/127], Loss: 0.00866959\n",
      "Epoch [71/100], Step [30/127], Loss: 0.00439912\n",
      "Epoch [71/100], Step [40/127], Loss: 0.00480821\n",
      "Epoch [71/100], Step [50/127], Loss: 0.01278250\n",
      "Epoch [71/100], Step [60/127], Loss: 0.00470043\n",
      "Epoch [71/100], Step [70/127], Loss: 0.00242074\n",
      "Epoch [71/100], Step [80/127], Loss: 0.01544334\n",
      "Epoch [71/100], Step [90/127], Loss: 0.00300453\n",
      "Epoch [71/100], Step [100/127], Loss: 0.00522048\n",
      "Epoch [71/100], Step [110/127], Loss: 0.00251719\n",
      "Epoch [71/100], Step [120/127], Loss: 0.00172574\n",
      "Epoch 71/100: Val Loss: 0.20815673\n",
      "Epoch [72/100], Step [10/127], Loss: 0.00473241\n",
      "Epoch [72/100], Step [20/127], Loss: 0.01251002\n",
      "Epoch [72/100], Step [30/127], Loss: 0.01097141\n",
      "Epoch [72/100], Step [40/127], Loss: 0.06009147\n",
      "Epoch [72/100], Step [50/127], Loss: 0.53550166\n",
      "Epoch [72/100], Step [60/127], Loss: 2.73395443\n",
      "Epoch [72/100], Step [70/127], Loss: 0.80678260\n",
      "Epoch [72/100], Step [80/127], Loss: 0.28024215\n",
      "Epoch [72/100], Step [90/127], Loss: 0.02122880\n",
      "Epoch [72/100], Step [100/127], Loss: 0.05742775\n",
      "Epoch [72/100], Step [110/127], Loss: 0.01208663\n",
      "Epoch [72/100], Step [120/127], Loss: 0.01626493\n",
      "Epoch 72/100: Val Loss: 0.26302221\n",
      "Epoch [73/100], Step [10/127], Loss: 0.07226440\n",
      "Epoch [73/100], Step [20/127], Loss: 0.29791796\n",
      "Epoch [73/100], Step [30/127], Loss: 0.05832940\n",
      "Epoch [73/100], Step [40/127], Loss: 0.01790534\n",
      "Epoch [73/100], Step [50/127], Loss: 0.01839296\n",
      "Epoch [73/100], Step [60/127], Loss: 0.00852406\n",
      "Epoch [73/100], Step [70/127], Loss: 0.00888226\n",
      "Epoch [73/100], Step [80/127], Loss: 0.01254801\n",
      "Epoch [73/100], Step [90/127], Loss: 0.01164929\n",
      "Epoch [73/100], Step [100/127], Loss: 0.04415887\n",
      "Epoch [73/100], Step [110/127], Loss: 0.01100985\n",
      "Epoch [73/100], Step [120/127], Loss: 0.02484861\n",
      "Epoch 73/100: Val Loss: 0.18450819\n",
      "Epoch [74/100], Step [10/127], Loss: 0.00525359\n",
      "Epoch [74/100], Step [20/127], Loss: 0.00526649\n",
      "Epoch [74/100], Step [30/127], Loss: 0.02981245\n",
      "Epoch [74/100], Step [40/127], Loss: 0.01325487\n",
      "Epoch [74/100], Step [50/127], Loss: 0.00913789\n",
      "Epoch [74/100], Step [60/127], Loss: 0.00238798\n",
      "Epoch [74/100], Step [70/127], Loss: 0.00474574\n",
      "Epoch [74/100], Step [80/127], Loss: 0.00546583\n",
      "Epoch [74/100], Step [90/127], Loss: 0.00284407\n",
      "Epoch [74/100], Step [100/127], Loss: 0.00645454\n",
      "Epoch [74/100], Step [110/127], Loss: 0.02285641\n",
      "Epoch [74/100], Step [120/127], Loss: 0.06025188\n",
      "Epoch 74/100: Val Loss: 0.27692820\n",
      "Epoch [75/100], Step [10/127], Loss: 0.06679805\n",
      "Epoch [75/100], Step [20/127], Loss: 0.16130744\n",
      "Epoch [75/100], Step [30/127], Loss: 5.71578026\n",
      "Epoch [75/100], Step [40/127], Loss: 3.93769598\n",
      "Epoch [75/100], Step [50/127], Loss: 2.12541270\n",
      "Epoch [75/100], Step [60/127], Loss: 2.85307670\n",
      "Epoch [75/100], Step [70/127], Loss: 0.84044629\n",
      "Epoch [75/100], Step [80/127], Loss: 0.17634308\n",
      "Epoch [75/100], Step [90/127], Loss: 0.07779971\n",
      "Epoch [75/100], Step [100/127], Loss: 0.06380128\n",
      "Epoch [75/100], Step [110/127], Loss: 0.04142286\n",
      "Epoch [75/100], Step [120/127], Loss: 0.01020344\n",
      "Epoch 75/100: Val Loss: 0.65712153\n",
      "Epoch [76/100], Step [10/127], Loss: 0.01313618\n",
      "Epoch [76/100], Step [20/127], Loss: 0.00460049\n",
      "Epoch [76/100], Step [30/127], Loss: 0.01240725\n",
      "Epoch [76/100], Step [40/127], Loss: 0.05188084\n",
      "Epoch [76/100], Step [50/127], Loss: 0.10330617\n",
      "Epoch [76/100], Step [60/127], Loss: 0.00353967\n",
      "Epoch [76/100], Step [70/127], Loss: 0.01330868\n",
      "Epoch [76/100], Step [80/127], Loss: 0.00410878\n",
      "Epoch [76/100], Step [90/127], Loss: 0.00936949\n",
      "Epoch [76/100], Step [100/127], Loss: 0.02021511\n",
      "Epoch [76/100], Step [110/127], Loss: 0.02555905\n",
      "Epoch [76/100], Step [120/127], Loss: 0.01719839\n",
      "Epoch 76/100: Val Loss: 0.37386188\n",
      "Epoch [77/100], Step [10/127], Loss: 0.06024345\n",
      "Epoch [77/100], Step [20/127], Loss: 0.01366711\n",
      "Epoch [77/100], Step [30/127], Loss: 0.02662491\n",
      "Epoch [77/100], Step [40/127], Loss: 0.00927673\n",
      "Epoch [77/100], Step [50/127], Loss: 0.00355890\n",
      "Epoch [77/100], Step [60/127], Loss: 0.02094088\n",
      "Epoch [77/100], Step [70/127], Loss: 0.00210290\n",
      "Epoch [77/100], Step [80/127], Loss: 0.00696853\n",
      "Epoch [77/100], Step [90/127], Loss: 0.00720986\n",
      "Epoch [77/100], Step [100/127], Loss: 0.00268503\n",
      "Epoch [77/100], Step [110/127], Loss: 0.00819346\n",
      "Epoch [77/100], Step [120/127], Loss: 0.01232040\n",
      "Epoch 77/100: Val Loss: 0.15075546\n",
      "Epoch [78/100], Step [10/127], Loss: 0.00328622\n",
      "Epoch [78/100], Step [20/127], Loss: 0.01012157\n",
      "Epoch [78/100], Step [30/127], Loss: 0.00281219\n",
      "Epoch [78/100], Step [40/127], Loss: 0.02828380\n",
      "Epoch [78/100], Step [50/127], Loss: 0.33854249\n",
      "Epoch [78/100], Step [60/127], Loss: 0.02996585\n",
      "Epoch [78/100], Step [70/127], Loss: 3.51844001\n",
      "Epoch [78/100], Step [80/127], Loss: 1.87069416\n",
      "Epoch [78/100], Step [90/127], Loss: 2.38656116\n",
      "Epoch [78/100], Step [100/127], Loss: 0.98690403\n",
      "Epoch [78/100], Step [110/127], Loss: 3.96207762\n",
      "Epoch [78/100], Step [120/127], Loss: 0.06639711\n",
      "Epoch 78/100: Val Loss: 0.21553363\n",
      "Epoch [79/100], Step [10/127], Loss: 0.10054895\n",
      "Epoch [79/100], Step [20/127], Loss: 0.07321513\n",
      "Epoch [79/100], Step [30/127], Loss: 0.01118669\n",
      "Epoch [79/100], Step [40/127], Loss: 0.04415596\n",
      "Epoch [79/100], Step [50/127], Loss: 0.01236812\n",
      "Epoch [79/100], Step [60/127], Loss: 0.03141689\n",
      "Epoch [79/100], Step [70/127], Loss: 0.05252633\n",
      "Epoch [79/100], Step [80/127], Loss: 0.08577140\n",
      "Epoch [79/100], Step [90/127], Loss: 0.01184526\n",
      "Epoch [79/100], Step [100/127], Loss: 0.03474794\n",
      "Epoch [79/100], Step [110/127], Loss: 0.00505265\n",
      "Epoch [79/100], Step [120/127], Loss: 0.00655288\n",
      "Epoch 79/100: Val Loss: 0.15486108\n",
      "Epoch [80/100], Step [10/127], Loss: 0.00975535\n",
      "Epoch [80/100], Step [20/127], Loss: 0.00515100\n",
      "Epoch [80/100], Step [30/127], Loss: 0.01189398\n",
      "Epoch [80/100], Step [40/127], Loss: 0.00690727\n",
      "Epoch [80/100], Step [50/127], Loss: 0.00620433\n",
      "Epoch [80/100], Step [60/127], Loss: 0.00428629\n",
      "Epoch [80/100], Step [70/127], Loss: 0.00285696\n",
      "Epoch [80/100], Step [80/127], Loss: 0.00251876\n",
      "Epoch [80/100], Step [90/127], Loss: 0.00561913\n",
      "Epoch [80/100], Step [100/127], Loss: 0.00671803\n",
      "Epoch [80/100], Step [110/127], Loss: 0.00763498\n",
      "Epoch [80/100], Step [120/127], Loss: 0.00631605\n",
      "Epoch 80/100: Val Loss: 0.17749006\n",
      "Epoch [81/100], Step [10/127], Loss: 0.00762757\n",
      "Epoch [81/100], Step [20/127], Loss: 0.00827900\n",
      "Epoch [81/100], Step [30/127], Loss: 0.00504898\n",
      "Epoch [81/100], Step [40/127], Loss: 0.02920887\n",
      "Epoch [81/100], Step [50/127], Loss: 0.00470910\n",
      "Epoch [81/100], Step [60/127], Loss: 0.00949112\n",
      "Epoch [81/100], Step [70/127], Loss: 0.01302006\n",
      "Epoch [81/100], Step [80/127], Loss: 0.00869690\n",
      "Epoch [81/100], Step [90/127], Loss: 0.00742873\n",
      "Epoch [81/100], Step [100/127], Loss: 0.00236173\n",
      "Epoch [81/100], Step [110/127], Loss: 0.00626384\n",
      "Epoch [81/100], Step [120/127], Loss: 0.02041469\n",
      "Epoch 81/100: Val Loss: 0.15878435\n",
      "Epoch [82/100], Step [10/127], Loss: 0.06185067\n",
      "Epoch [82/100], Step [20/127], Loss: 0.00957378\n",
      "Epoch [82/100], Step [30/127], Loss: 0.01882344\n",
      "Epoch [82/100], Step [40/127], Loss: 0.00466152\n",
      "Epoch [82/100], Step [50/127], Loss: 0.00425427\n",
      "Epoch [82/100], Step [60/127], Loss: 0.00677985\n",
      "Epoch [82/100], Step [70/127], Loss: 0.00339072\n",
      "Epoch [82/100], Step [80/127], Loss: 0.00487694\n",
      "Epoch [82/100], Step [90/127], Loss: 0.00436109\n",
      "Epoch [82/100], Step [100/127], Loss: 0.00472305\n",
      "Epoch [82/100], Step [110/127], Loss: 0.01399648\n",
      "Epoch [82/100], Step [120/127], Loss: 0.03606813\n",
      "Epoch 82/100: Val Loss: 0.21074151\n",
      "Epoch [83/100], Step [10/127], Loss: 0.01388843\n",
      "Epoch [83/100], Step [20/127], Loss: 0.49573007\n",
      "Epoch [83/100], Step [30/127], Loss: 0.09481804\n",
      "Epoch [83/100], Step [40/127], Loss: 0.68308628\n",
      "Epoch [83/100], Step [50/127], Loss: 0.40382549\n",
      "Epoch [83/100], Step [60/127], Loss: 1.69136822\n",
      "Epoch [83/100], Step [70/127], Loss: 0.43649960\n",
      "Epoch [83/100], Step [80/127], Loss: 0.57483977\n",
      "Epoch [83/100], Step [90/127], Loss: 0.33422315\n",
      "Epoch [83/100], Step [100/127], Loss: 0.13894463\n",
      "Epoch [83/100], Step [110/127], Loss: 0.33731791\n",
      "Epoch [83/100], Step [120/127], Loss: 0.04809985\n",
      "Epoch 83/100: Val Loss: 0.33908445\n",
      "Epoch [84/100], Step [10/127], Loss: 0.01261449\n",
      "Epoch [84/100], Step [20/127], Loss: 0.02060349\n",
      "Epoch [84/100], Step [30/127], Loss: 0.02267052\n",
      "Epoch [84/100], Step [40/127], Loss: 0.09855843\n",
      "Epoch [84/100], Step [50/127], Loss: 0.02656536\n",
      "Epoch [84/100], Step [60/127], Loss: 0.15698977\n",
      "Epoch [84/100], Step [70/127], Loss: 0.03317656\n",
      "Epoch [84/100], Step [80/127], Loss: 0.05127382\n",
      "Epoch [84/100], Step [90/127], Loss: 0.08521415\n",
      "Epoch [84/100], Step [100/127], Loss: 0.02643782\n",
      "Epoch [84/100], Step [110/127], Loss: 0.01108661\n",
      "Epoch [84/100], Step [120/127], Loss: 0.04123603\n",
      "Epoch 84/100: Val Loss: 0.48579955\n",
      "Epoch [85/100], Step [10/127], Loss: 0.02539295\n",
      "Epoch [85/100], Step [20/127], Loss: 0.01043641\n",
      "Epoch [85/100], Step [30/127], Loss: 0.00829647\n",
      "Epoch [85/100], Step [40/127], Loss: 0.06804734\n",
      "Epoch [85/100], Step [50/127], Loss: 0.05981940\n",
      "Epoch [85/100], Step [60/127], Loss: 0.02329865\n",
      "Epoch [85/100], Step [70/127], Loss: 0.00942735\n",
      "Epoch [85/100], Step [80/127], Loss: 0.02518257\n",
      "Epoch [85/100], Step [90/127], Loss: 0.14839555\n",
      "Epoch [85/100], Step [100/127], Loss: 0.03758129\n",
      "Epoch [85/100], Step [110/127], Loss: 0.02729553\n",
      "Epoch [85/100], Step [120/127], Loss: 0.04593224\n",
      "Epoch 85/100: Val Loss: 0.87211300\n",
      "Epoch [86/100], Step [10/127], Loss: 0.04171675\n",
      "Epoch [86/100], Step [20/127], Loss: 0.02980889\n",
      "Epoch [86/100], Step [30/127], Loss: 0.01396269\n",
      "Epoch [86/100], Step [40/127], Loss: 0.02760566\n",
      "Epoch [86/100], Step [50/127], Loss: 0.01701189\n",
      "Epoch [86/100], Step [60/127], Loss: 0.01804724\n",
      "Epoch [86/100], Step [70/127], Loss: 0.02854633\n",
      "Epoch [86/100], Step [80/127], Loss: 0.01297533\n",
      "Epoch [86/100], Step [90/127], Loss: 0.02170281\n",
      "Epoch [86/100], Step [100/127], Loss: 0.01630827\n",
      "Epoch [86/100], Step [110/127], Loss: 0.10613138\n",
      "Epoch [86/100], Step [120/127], Loss: 0.01555322\n",
      "Epoch 86/100: Val Loss: 0.38648777\n",
      "Epoch [87/100], Step [10/127], Loss: 0.12867272\n",
      "Epoch [87/100], Step [20/127], Loss: 0.20427491\n",
      "Epoch [87/100], Step [30/127], Loss: 0.06488832\n",
      "Epoch [87/100], Step [40/127], Loss: 0.18532413\n",
      "Epoch [87/100], Step [50/127], Loss: 0.14334951\n",
      "Epoch [87/100], Step [60/127], Loss: 0.15915252\n",
      "Epoch [87/100], Step [70/127], Loss: 0.05073860\n",
      "Epoch [87/100], Step [80/127], Loss: 0.25330788\n",
      "Epoch [87/100], Step [90/127], Loss: 0.07167447\n",
      "Epoch [87/100], Step [100/127], Loss: 0.06708971\n",
      "Epoch [87/100], Step [110/127], Loss: 0.14678405\n",
      "Epoch [87/100], Step [120/127], Loss: 0.17107268\n",
      "Epoch 87/100: Val Loss: 0.23419361\n",
      "Epoch [88/100], Step [10/127], Loss: 0.28554556\n",
      "Epoch [88/100], Step [20/127], Loss: 0.07771664\n",
      "Epoch [88/100], Step [30/127], Loss: 0.08564622\n",
      "Epoch [88/100], Step [40/127], Loss: 0.02405941\n",
      "Epoch [88/100], Step [50/127], Loss: 0.02075814\n",
      "Epoch [88/100], Step [60/127], Loss: 0.00998546\n",
      "Epoch [88/100], Step [70/127], Loss: 0.01507747\n",
      "Epoch [88/100], Step [80/127], Loss: 0.03296657\n",
      "Epoch [88/100], Step [90/127], Loss: 0.02704229\n",
      "Epoch [88/100], Step [100/127], Loss: 0.03281809\n",
      "Epoch [88/100], Step [110/127], Loss: 0.07324988\n",
      "Epoch [88/100], Step [120/127], Loss: 0.00713297\n",
      "Epoch 88/100: Val Loss: 0.17258372\n",
      "Epoch [89/100], Step [10/127], Loss: 0.04765212\n",
      "Epoch [89/100], Step [20/127], Loss: 1.14659357\n",
      "Epoch [89/100], Step [30/127], Loss: 2.20524836\n",
      "Epoch [89/100], Step [40/127], Loss: 0.54444212\n",
      "Epoch [89/100], Step [50/127], Loss: 0.01591746\n",
      "Epoch [89/100], Step [60/127], Loss: 0.17353550\n",
      "Epoch [89/100], Step [70/127], Loss: 0.23034973\n",
      "Epoch [89/100], Step [80/127], Loss: 0.09229627\n",
      "Epoch [89/100], Step [90/127], Loss: 0.07080384\n",
      "Epoch [89/100], Step [100/127], Loss: 0.02282770\n",
      "Epoch [89/100], Step [110/127], Loss: 0.00615976\n",
      "Epoch [89/100], Step [120/127], Loss: 0.02612204\n",
      "Epoch 89/100: Val Loss: 0.21644232\n",
      "Epoch [90/100], Step [10/127], Loss: 0.11643540\n",
      "Epoch [90/100], Step [20/127], Loss: 0.01757637\n",
      "Epoch [90/100], Step [30/127], Loss: 0.06496403\n",
      "Epoch [90/100], Step [40/127], Loss: 0.02687003\n",
      "Epoch [90/100], Step [50/127], Loss: 0.00689821\n",
      "Epoch [90/100], Step [60/127], Loss: 0.01162210\n",
      "Epoch [90/100], Step [70/127], Loss: 0.02449842\n",
      "Epoch [90/100], Step [80/127], Loss: 0.07188889\n",
      "Epoch [90/100], Step [90/127], Loss: 0.04393476\n",
      "Epoch [90/100], Step [100/127], Loss: 0.02661267\n",
      "Epoch [90/100], Step [110/127], Loss: 0.03897102\n",
      "Epoch [90/100], Step [120/127], Loss: 0.01806504\n",
      "Epoch 90/100: Val Loss: 0.14474400\n",
      "Epoch [91/100], Step [10/127], Loss: 0.01631263\n",
      "Epoch [91/100], Step [20/127], Loss: 0.02492232\n",
      "Epoch [91/100], Step [30/127], Loss: 0.10726941\n",
      "Epoch [91/100], Step [40/127], Loss: 0.00888926\n",
      "Epoch [91/100], Step [50/127], Loss: 0.01428418\n",
      "Epoch [91/100], Step [60/127], Loss: 0.00532023\n",
      "Epoch [91/100], Step [70/127], Loss: 0.23829818\n",
      "Epoch [91/100], Step [80/127], Loss: 0.01446722\n",
      "Epoch [91/100], Step [90/127], Loss: 0.08096825\n",
      "Epoch [91/100], Step [100/127], Loss: 0.64062291\n",
      "Epoch [91/100], Step [110/127], Loss: 0.26524132\n",
      "Epoch [91/100], Step [120/127], Loss: 0.69632149\n",
      "Epoch 91/100: Val Loss: 0.20892733\n",
      "Epoch [92/100], Step [10/127], Loss: 0.14639778\n",
      "Epoch [92/100], Step [20/127], Loss: 0.01495394\n",
      "Epoch [92/100], Step [30/127], Loss: 0.05642157\n",
      "Epoch [92/100], Step [40/127], Loss: 0.01852521\n",
      "Epoch [92/100], Step [50/127], Loss: 0.00609060\n",
      "Epoch [92/100], Step [60/127], Loss: 0.00954199\n",
      "Epoch [92/100], Step [70/127], Loss: 0.01849280\n",
      "Epoch [92/100], Step [80/127], Loss: 0.02473903\n",
      "Epoch [92/100], Step [90/127], Loss: 0.01527381\n",
      "Epoch [92/100], Step [100/127], Loss: 0.02225448\n",
      "Epoch [92/100], Step [110/127], Loss: 0.07211027\n",
      "Epoch [92/100], Step [120/127], Loss: 0.28439045\n",
      "Epoch 92/100: Val Loss: 0.26694636\n",
      "Epoch [93/100], Step [10/127], Loss: 0.09595057\n",
      "Epoch [93/100], Step [20/127], Loss: 0.24506722\n",
      "Epoch [93/100], Step [30/127], Loss: 0.04009284\n",
      "Epoch [93/100], Step [40/127], Loss: 0.08814409\n",
      "Epoch [93/100], Step [50/127], Loss: 0.05688159\n",
      "Epoch [93/100], Step [60/127], Loss: 0.13203259\n",
      "Epoch [93/100], Step [70/127], Loss: 0.01913245\n",
      "Epoch [93/100], Step [80/127], Loss: 0.02007160\n",
      "Epoch [93/100], Step [90/127], Loss: 0.03642313\n",
      "Epoch [93/100], Step [100/127], Loss: 0.01628817\n",
      "Epoch [93/100], Step [110/127], Loss: 0.01068295\n",
      "Epoch [93/100], Step [120/127], Loss: 0.03723510\n",
      "Epoch 93/100: Val Loss: 0.18502439\n",
      "Epoch [94/100], Step [10/127], Loss: 0.00727994\n",
      "Epoch [94/100], Step [20/127], Loss: 0.03890007\n",
      "Epoch [94/100], Step [30/127], Loss: 0.02525423\n",
      "Epoch [94/100], Step [40/127], Loss: 0.00432810\n",
      "Epoch [94/100], Step [50/127], Loss: 0.02629037\n",
      "Epoch [94/100], Step [60/127], Loss: 0.02886603\n",
      "Epoch [94/100], Step [70/127], Loss: 0.82394558\n",
      "Epoch [94/100], Step [80/127], Loss: 0.17046525\n",
      "Epoch [94/100], Step [90/127], Loss: 0.01818200\n",
      "Epoch [94/100], Step [100/127], Loss: 0.17388338\n",
      "Epoch [94/100], Step [110/127], Loss: 0.16682111\n",
      "Epoch [94/100], Step [120/127], Loss: 0.01913455\n",
      "Epoch 94/100: Val Loss: 0.27931164\n",
      "Epoch [95/100], Step [10/127], Loss: 0.01728637\n",
      "Epoch [95/100], Step [20/127], Loss: 0.01095609\n",
      "Epoch [95/100], Step [30/127], Loss: 0.01005281\n",
      "Epoch [95/100], Step [40/127], Loss: 0.10807452\n",
      "Epoch [95/100], Step [50/127], Loss: 0.05916829\n",
      "Epoch [95/100], Step [60/127], Loss: 0.02112180\n",
      "Epoch [95/100], Step [70/127], Loss: 0.03681745\n",
      "Epoch [95/100], Step [80/127], Loss: 0.27415329\n",
      "Epoch [95/100], Step [90/127], Loss: 0.06146765\n",
      "Epoch [95/100], Step [100/127], Loss: 0.05545184\n",
      "Epoch [95/100], Step [110/127], Loss: 0.03565627\n",
      "Epoch [95/100], Step [120/127], Loss: 0.33015668\n",
      "Epoch 95/100: Val Loss: 0.32741054\n",
      "Epoch [96/100], Step [10/127], Loss: 0.01492480\n",
      "Epoch [96/100], Step [20/127], Loss: 0.03635189\n",
      "Epoch [96/100], Step [30/127], Loss: 0.53525192\n",
      "Epoch [96/100], Step [40/127], Loss: 0.46271473\n",
      "Epoch [96/100], Step [50/127], Loss: 0.33914030\n",
      "Epoch [96/100], Step [60/127], Loss: 0.03387051\n",
      "Epoch [96/100], Step [70/127], Loss: 0.02229219\n",
      "Epoch [96/100], Step [80/127], Loss: 0.04867836\n",
      "Epoch [96/100], Step [90/127], Loss: 0.06267418\n",
      "Epoch [96/100], Step [100/127], Loss: 0.01543105\n",
      "Epoch [96/100], Step [110/127], Loss: 0.04780548\n",
      "Epoch [96/100], Step [120/127], Loss: 0.00971048\n",
      "Epoch 96/100: Val Loss: 0.23595911\n",
      "Epoch [97/100], Step [10/127], Loss: 0.01432237\n",
      "Epoch [97/100], Step [20/127], Loss: 0.05536013\n",
      "Epoch [97/100], Step [30/127], Loss: 0.04411754\n",
      "Epoch [97/100], Step [40/127], Loss: 0.01229790\n",
      "Epoch [97/100], Step [50/127], Loss: 0.01468643\n",
      "Epoch [97/100], Step [60/127], Loss: 0.02962800\n",
      "Epoch [97/100], Step [70/127], Loss: 0.03999530\n",
      "Epoch [97/100], Step [80/127], Loss: 0.01615286\n",
      "Epoch [97/100], Step [90/127], Loss: 0.00937284\n",
      "Epoch [97/100], Step [100/127], Loss: 0.02341814\n",
      "Epoch [97/100], Step [110/127], Loss: 0.00836638\n",
      "Epoch [97/100], Step [120/127], Loss: 0.01890329\n",
      "Epoch 97/100: Val Loss: 0.25673465\n",
      "Epoch [98/100], Step [10/127], Loss: 0.08559107\n",
      "Epoch [98/100], Step [20/127], Loss: 0.04018478\n",
      "Epoch [98/100], Step [30/127], Loss: 0.02200046\n",
      "Epoch [98/100], Step [40/127], Loss: 0.02439141\n",
      "Epoch [98/100], Step [50/127], Loss: 0.12636481\n",
      "Epoch [98/100], Step [60/127], Loss: 0.40112719\n",
      "Epoch [98/100], Step [70/127], Loss: 0.35467353\n",
      "Epoch [98/100], Step [80/127], Loss: 1.02499604\n",
      "Epoch [98/100], Step [90/127], Loss: 0.13878699\n",
      "Epoch [98/100], Step [100/127], Loss: 0.55221820\n",
      "Epoch [98/100], Step [110/127], Loss: 0.76233679\n",
      "Epoch [98/100], Step [120/127], Loss: 4.65343094\n",
      "Epoch 98/100: Val Loss: 0.30617853\n",
      "Epoch [99/100], Step [10/127], Loss: 0.73359436\n",
      "Epoch [99/100], Step [20/127], Loss: 0.23155248\n",
      "Epoch [99/100], Step [30/127], Loss: 0.15169264\n",
      "Epoch [99/100], Step [40/127], Loss: 0.04339749\n",
      "Epoch [99/100], Step [50/127], Loss: 0.05075580\n",
      "Epoch [99/100], Step [60/127], Loss: 0.43390003\n",
      "Epoch [99/100], Step [70/127], Loss: 0.05848741\n",
      "Epoch [99/100], Step [80/127], Loss: 0.08281288\n",
      "Epoch [99/100], Step [90/127], Loss: 0.02898819\n",
      "Epoch [99/100], Step [100/127], Loss: 0.00873173\n",
      "Epoch [99/100], Step [110/127], Loss: 0.01568006\n",
      "Epoch [99/100], Step [120/127], Loss: 0.00911263\n",
      "Epoch 99/100: Val Loss: 0.23720674\n",
      "Epoch [100/100], Step [10/127], Loss: 0.01461749\n",
      "Epoch [100/100], Step [20/127], Loss: 0.01245934\n",
      "Epoch [100/100], Step [30/127], Loss: 0.00449513\n",
      "Epoch [100/100], Step [40/127], Loss: 0.01081527\n",
      "Epoch [100/100], Step [50/127], Loss: 0.01028708\n",
      "Epoch [100/100], Step [60/127], Loss: 0.00893562\n",
      "Epoch [100/100], Step [70/127], Loss: 0.00541321\n",
      "Epoch [100/100], Step [80/127], Loss: 0.00721388\n",
      "Epoch [100/100], Step [90/127], Loss: 0.03046552\n",
      "Epoch [100/100], Step [100/127], Loss: 0.00269503\n",
      "Epoch [100/100], Step [110/127], Loss: 0.00896510\n",
      "Epoch [100/100], Step [120/127], Loss: 0.13656749\n",
      "Epoch 100/100: Val Loss: 0.31136078\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "total_steps = len(train_dataloader)\n",
    "val_loss_optimize = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        # 前向传播\n",
    "        # print(labels)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        # print(outputs.size())\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        # 打印训练信息\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{total_steps}], Loss: {loss.item():.8f}')\n",
    "            \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in val_dataloader:\n",
    "            # 前向传播\n",
    "            outputs = model(batch_data.to(device))\n",
    "#             print(f\"outputs:{outputs}\")\n",
    "#             print(f\"batch_labels:{batch_labels}\")\n",
    "            \n",
    "#             loss = torch.mean(torch.mean(torch.abs(outputs - batch_labels.to(device))/batch_labels.to(device),axis = 0),axis = 1)\n",
    "            loss = torch.mean(torch.mean(torch.abs(outputs - batch_labels.to(device))/batch_labels.to(device),axis = 1))\n",
    "#             print(loss.item())\n",
    "#             print(len(loss2))\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "#             print(val_loss)\n",
    "#             print(len(val_dataloader))\n",
    "            average_xiangdui_loss = val_loss / len(val_dataloader)\n",
    "#             print(f\"Val Loss: {average_xiangdui_loss:.8f}\")\n",
    "            # 计算误差\n",
    "        \n",
    "   \n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Val Loss: {average_xiangdui_loss:.8f}\")\n",
    "#     if average_xiangdui_loss < val_loss_optimize:\n",
    "#         torch.save(model.state_dict(), 'MLP_E_model_weights.pth202438')\n",
    "#         val_loss_optimize = average_xiangdui_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed1aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "input = val_data[5],label = val_labels[5]\n",
    "output = model(input)\n",
    "end = time.time()\n",
    "print(f\"time cost {end - start}s\")\n",
    "with open(\"time_cost_MLP.txt\", \"w\") as file:\n",
    "    file.write(str(end - start))  # 将变量转换为字符串并写入文件\n",
    "    \n",
    "error = torch.mean(torch.abs(output - label)/label,axis = 1)\n",
    "np.save('MLP_error.npy', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953ad77-0d0b-452c-b862-6336bc9c5674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLP-E",
   "language": "python",
   "name": "mlp-e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
